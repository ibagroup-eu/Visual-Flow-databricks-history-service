{
  "id": 8660172650147071349,
  "timestamp": 12345,
  "jobLogs": [
    {
      "timestamp": "24/07/02 20:00:19",
      "level": "INFO",
      "message": "DriverDaemon$: Started Log4j2"
    },
    {
      "timestamp": "24/07/02 20:00:21",
      "level": "INFO",
      "message": "DriverDaemon$: Current JVM Version 11.0.21"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: ========== driver starting up =========="
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: Java: Azul Systems, Inc. 11.0.21"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: OS: Linux/amd64 5.15.0-1063-aws"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: CWD: /databricks/driver"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: Mem: Max: 8.2G loaded GCs: G1 Young Generation, G1 Old Generation"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: Logging multibyte characters: âœ“"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: 'publicFile.rolling.rewrite' appender in root logger: class org.apache.logging.log4j.core.appender.rewrite.RewriteAppender"
    },
    {
      "timestamp": "24/07/02 20:00:22",
      "level": "INFO",
      "message": "DriverDaemon$: == Modules:"
    },
    {
      "timestamp": "24/07/02 20:00:23",
      "level": "INFO",
      "message": "DynamicLoggingConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:23",
      "level": "WARN",
      "message": "DynamicLoggingConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:23",
      "level": "INFO",
      "message": "FeatureFlagRegisterConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:23",
      "level": "WARN",
      "message": "FeatureFlagRegisterConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "DriverDaemon$: Starting prometheus metrics log export timer"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "DriverConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "WARN",
      "message": "DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "DriverDaemon$: Skipping class preloading because it is not enabled in conf"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "DriverDaemon$: Loaded JDBC drivers in 352 ms"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "DriverDaemon$: Universe Git Hash: 9dc12ff30c0c65064dc2ace667593886e967f891"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "DriverDaemon$: Spark Git Hash: affbff1ea6ec4937bc3a7475bf5fa8e882ce2c5d"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.password"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path"
    },
    {
      "timestamp": "24/07/02 20:00:24",
      "level": "INFO",
      "message": "SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "WARN",
      "message": "RunHelpers$: Missing tag isolation client: java.util.NoSuchElementException: key not found: TagDefinition(clientType,The client type for a request, used for isolating resources for the request.,DATA_LABEL_SYSTEM_NOT_SENSITIVE,false,false,List(),UsageLogRedactionConfig(List()))"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "MetastoreMonitor$: Internal metastore configured"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "DatabricksILoop$: Creating throwaway interpreter"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "DataSourceFactory$: DataSource Jdbc URL: jdbc:mariadb://mdb7sywh50xhpr.chkweekm4xjq.us-east-1.rds.amazonaws.com:3306/organization42883594658554?useSSL=true&sslMode=VERIFY_CA&disableSslHostnameVerification=true&trustServerCertificate=false&serverSslCert=/databricks/common/mysql-ssl-ca-cert.crt"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "NestedConnectionMonitor$$anon$1: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "WARN",
      "message": "NestedConnectionMonitor$$anon$1: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "DriverCorral: Creating the driver context"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "DatabricksILoop$: Class Server Dir: /local_disk0/tmp/repl/spark-153907360229993971-44272e49-f0a6-400a-be29-d8b892db0d30"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "HikariDataSource: metastore-monitor - Starting..."
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.password"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf"
    },
    {
      "timestamp": "24/07/02 20:00:25",
      "level": "INFO",
      "message": "HikariDataSource: metastore-monitor - Start completed."
    },
    {
      "timestamp": "24/07/02 20:00:26",
      "level": "INFO",
      "message": "SparkContext: Running Spark version 3.4.1"
    },
    {
      "timestamp": "24/07/02 20:00:26",
      "level": "INFO",
      "message": "SparkContext: OS info Linux, 5.15.0-1063-aws, amd64"
    },
    {
      "timestamp": "24/07/02 20:00:26",
      "level": "INFO",
      "message": "SparkContext: Java version 11.0.21"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "DatabricksEdgeConfigs: serverlessEnabled : false"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "DatabricksEdgeConfigs: perfPackEnabled : true"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "DatabricksEdgeConfigs: classicSqlEnabled : true"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "ResourceUtils: =============================================================="
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "ResourceUtils: No custom resources configured for spark.driver."
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "ResourceUtils: =============================================================="
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "SparkContext: Submitted application: Databricks Shell"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 2218, script: , vendor: , offHeap -> name: offHeap, amount: 6655, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "ResourceProfile: Limiting resource is cpu"
    },
    {
      "timestamp": "24/07/02 20:00:27",
      "level": "INFO",
      "message": "ResourceProfileManager: Added ResourceProfile id: 0"
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "HikariDataSource: metastore-monitor - Shutdown initiated..."
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "HikariDataSource: metastore-monitor - Shutdown completed."
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "MetastoreMonitor: Metastore healthcheck successful (connection duration = 2720 milliseconds)"
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "SecurityManager: Changing view acls to: root"
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "SecurityManager: Changing modify acls to: root"
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "SecurityManager: Changing view acls groups to: "
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "SecurityManager: Changing modify acls groups to: "
    },
    {
      "timestamp": "24/07/02 20:00:28",
      "level": "INFO",
      "message": "SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "Utils: Successfully started service 'sparkDriver' on port 40595."
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SparkEnv: Registering MapOutputTracker"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SecurityManager: Changing view acls to: root"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SecurityManager: Changing modify acls to: root"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SecurityManager: Changing view acls groups to: "
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SecurityManager: Changing modify acls groups to: "
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY; RPC SSL disabled"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SparkEnv: Registering BlockManagerMaster"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up"
    },
    {
      "timestamp": "24/07/02 20:00:29",
      "level": "INFO",
      "message": "SparkEnv: Registering BlockManagerMasterHeartbeat"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "DiskBlockManager: Created local directory at /local_disk0/blockmgr-dcf88307-1a86-4e7d-b6e0-81ac281fae8d"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "MemoryStore: MemoryStore started with capacity 10.9 GiB"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkEnv: Registering OutputCommitCoordinator"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkContext: Spark configuration:\nlibraryDownload.sleepIntervalSeconds=5\nlibraryDownload.timeoutSeconds=180\nspark.akka.frameSize=256\nspark.app.name=Databricks Shell\nspark.app.startTime=1719950426134\nspark.cleaner.referenceTracking.blocking=false\nspark.databricks.acl.client=com.databricks.spark.sql.acl.client.SparkSqlAclClient\nspark.databricks.acl.provider=com.databricks.sql.acl.ReflectionBackedAclProvider\nspark.databricks.acl.scim.client=com.databricks.spark.sql.acl.client.DriverToWebappScimClient\nspark.databricks.automl.serviceEnabled=true\nspark.databricks.autotune.maintenance.client.classname=com.databricks.maintenanceautocompute.MACClientImpl\nspark.databricks.cloudProvider=AWS\nspark.databricks.cloudfetch.hasRegionSupport=true\nspark.databricks.cloudfetch.requestDownloadUrlsWithHeaders=*********(redacted)\nspark.databricks.cloudfetch.requesterClassName=*********(redacted)\nspark.databricks.clusterSource=JOB\nspark.databricks.clusterUsageTags.attribute_tag_budget=\nspark.databricks.clusterUsageTags.attribute_tag_dust_execution_env=\nspark.databricks.clusterUsageTags.attribute_tag_dust_maintainer=\nspark.databricks.clusterUsageTags.attribute_tag_dust_suite=\nspark.databricks.clusterUsageTags.attribute_tag_service=\nspark.databricks.clusterUsageTags.autoTerminationMinutes=0\nspark.databricks.clusterUsageTags.awsWorkspaceIMDSV2EnablementStatus=false\nspark.databricks.clusterUsageTags.cloudProvider=AWS\nspark.databricks.clusterUsageTags.clusterAllTags=[{\"key\":\"Vendor\",\"value\":\"Databricks\"},{\"key\":\"Creator\",\"value\":\"datamanchuk@ibagroup.eu\"},{\"key\":\"ClusterName\",\"value\":\"job-208451582985809-run-710061071050445\"},{\"key\":\"ClusterId\",\"value\":\"0702-195546-nr4ndckt\"},{\"key\":\"JobId\",\"value\":\"208451582985809\"},{\"key\":\"RunName\",\"value\":\"demo-text\"},{\"key\":\"Name\",\"value\":\"workerenv-42883594658554-5b4dc299-d894-45b4-a5ee-5e7ccdc4ce90-worker\"}]\nspark.databricks.clusterUsageTags.clusterAvailability=SPOT_WITH_FALLBACK\nspark.databricks.clusterUsageTags.clusterCreator=JobLauncher\nspark.databricks.clusterUsageTags.clusterEbsVolumeCount=0\nspark.databricks.clusterUsageTags.clusterEbsVolumeSize=0\nspark.databricks.clusterUsageTags.clusterEbsVolumeType=GENERAL_PURPOSE_SSD\nspark.databricks.clusterUsageTags.clusterFirstOnDemand=1\nspark.databricks.clusterUsageTags.clusterGeneration=0\nspark.databricks.clusterUsageTags.clusterId=0702-195546-nr4ndckt\nspark.databricks.clusterUsageTags.clusterJdkBuildNameEnv=zulu11-ca-amd64\nspark.databricks.clusterUsageTags.clusterLogDeliveryEnabled=true\nspark.databricks.clusterUsageTags.clusterLogDestination=dbfs:/logStore/log\nspark.databricks.clusterUsageTags.clusterMetastoreAccessType=RDS_DIRECT\nspark.databricks.clusterUsageTags.clusterName=job-208451582985809-run-710061071050445\nspark.databricks.clusterUsageTags.clusterNoDriverDaemon=false\nspark.databricks.clusterUsageTags.clusterNodeType=r5d.large\nspark.databricks.clusterUsageTags.clusterNumCustomTags=0\nspark.databricks.clusterUsageTags.clusterNumSshKeys=0\nspark.databricks.clusterUsageTags.clusterOwnerOrgId=42883594658554\nspark.databricks.clusterUsageTags.clusterOwnerUserId=*********(redacted)\nspark.databricks.clusterUsageTags.clusterPinned=false\nspark.databricks.clusterUsageTags.clusterPythonVersion=3\nspark.databricks.clusterUsageTags.clusterResourceClass=default\nspark.databricks.clusterUsageTags.clusterScalingType=fixed_size\nspark.databricks.clusterUsageTags.clusterSizeType=VM_CONTAINER\nspark.databricks.clusterUsageTags.clusterSku=STANDARD_SKU\nspark.databricks.clusterUsageTags.clusterSpotBidPricePercent=100\nspark.databricks.clusterUsageTags.clusterState=Pending\nspark.databricks.clusterUsageTags.clusterStateMessage=Starting Spark\nspark.databricks.clusterUsageTags.clusterTargetWorkers=1\nspark.databricks.clusterUsageTags.clusterUnityCatalogMode=*********(redacted)\nspark.databricks.clusterUsageTags.clusterWorkers=1\nspark.databricks.clusterUsageTags.containerType=LXC\nspark.databricks.clusterUsageTags.containerZoneId=auto\nspark.databricks.clusterUsageTags.currentAttemptContainerZoneId=us-east-1d\nspark.databricks.clusterUsageTags.dataPlaneRegion=us-east-1\nspark.databricks.clusterUsageTags.driverContainerId=4f0e055838394845ae2f228b65bb97ce\nspark.databricks.clusterUsageTags.driverContainerPrivateIp=10.89.232.166\nspark.databricks.clusterUsageTags.driverInstanceId=i-0efa059c748dd4582\nspark.databricks.clusterUsageTags.driverInstancePrivateIp=10.89.248.30\nspark.databricks.clusterUsageTags.driverNodeType=r5d.large\nspark.databricks.clusterUsageTags.effectiveSparkVersion=13.3.x-photon-scala2.12\nspark.databricks.clusterUsageTags.enableCredentialPassthrough=*********(redacted)\nspark.databricks.clusterUsageTags.enableDfAcls=false\nspark.databricks.clusterUsageTags.enableElasticDisk=false\nspark.databricks.clusterUsageTags.enableGlueCatalogCredentialPassthrough=*********(redacted)\nspark.databricks.clusterUsageTags.enableJdbcAutoStart=true\nspark.databricks.clusterUsageTags.enableJobsAutostart=true\nspark.databricks.clusterUsageTags.enableLocalDiskEncryption=false\nspark.databricks.clusterUsageTags.enableSqlAclsOnly=false\nspark.databricks.clusterUsageTags.hailEnabled=false\nspark.databricks.clusterUsageTags.ignoreTerminationEventInAlerting=false\nspark.databricks.clusterUsageTags.instanceBootstrapType=vm-selfbootstrap\nspark.databricks.clusterUsageTags.instanceProfileUsed=false\nspark.databricks.clusterUsageTags.instanceWorkerEnvId=workerenv-42883594658554-5b4dc299-d894-45b4-a5ee-5e7ccdc4ce90\nspark.databricks.clusterUsageTags.instanceWorkerEnvNetworkType=default\nspark.databricks.clusterUsageTags.isDpCpPrivateLinkEnabled=false\nspark.databricks.clusterUsageTags.isIMv2Enabled=true\nspark.databricks.clusterUsageTags.isServicePrincipalCluster=false\nspark.databricks.clusterUsageTags.isSingleUserCluster=*********(redacted)\nspark.databricks.clusterUsageTags.ngrokNpipEnabled=true\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Abfss=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Dbfs=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2File=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Gcs=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2S3=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Volumes=0\nspark.databricks.clusterUsageTags.numPerClusterInitScriptsV2Workspace=0\nspark.databricks.clusterUsageTags.numPerGlobalInitScriptsV2=0\nspark.databricks.clusterUsageTags.orgId=42883594658554\nspark.databricks.clusterUsageTags.privateLinkEnabled=false\nspark.databricks.clusterUsageTags.region=us-east-1\nspark.databricks.clusterUsageTags.runtimeEngine=PHOTON\nspark.databricks.clusterUsageTags.sparkEnvVarContainsBacktick=false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDollarSign=false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsDoubleQuotes=false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsEscape=false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsNewline=false\nspark.databricks.clusterUsageTags.sparkEnvVarContainsSingleQuotes=false\nspark.databricks.clusterUsageTags.sparkImageLabel=release__13.3.x-snapshot-photon-scala2.12__databricks-universe__13.3.19__9dc12ff__affbff1__jenkins__9771c0e__format-3\nspark.databricks.clusterUsageTags.sparkMasterUrlType=*********(redacted)\nspark.databricks.clusterUsageTags.sparkVersion=13.3.x-photon-scala2.12\nspark.databricks.clusterUsageTags.userId=*********(redacted)\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeCount=*********(redacted)\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeSizeGb=*********(redacted)\nspark.databricks.clusterUsageTags.userProvidedRemoteVolumeType=*********(redacted)\nspark.databricks.clusterUsageTags.userProvidedSparkVersion=*********(redacted)\nspark.databricks.clusterUsageTags.workerEnvironmentId=workerenv-42883594658554-5b4dc299-d894-45b4-a5ee-5e7ccdc4ce90\nspark.databricks.credential.aws.secretKey.redactor=*********(redacted)\nspark.databricks.credential.redactor=*********(redacted)\nspark.databricks.credential.scope.fs.adls.gen2.tokenProviderClassName=*********(redacted)\nspark.databricks.credential.scope.fs.gs.auth.access.tokenProviderClassName=*********(redacted)\nspark.databricks.credential.scope.fs.impl=*********(redacted)\nspark.databricks.credential.scope.fs.s3a.tokenProviderClassName=*********(redacted)\nspark.databricks.delta.logStore.crossCloud.fatal=true\nspark.databricks.delta.multiClusterWrites.enabled=true\nspark.databricks.driver.preferredMavenCentralMirrorUrl=*********(redacted)\nspark.databricks.driverNfs.clusterWidePythonLibsEnabled=true\nspark.databricks.driverNfs.enabled=true\nspark.databricks.driverNfs.pathSuffix=.ephemeral_nfs\nspark.databricks.driverNodeTypeId=r5d.large\nspark.databricks.enablePublicDbfsFuse=false\nspark.databricks.eventLog.dir=eventlogs\nspark.databricks.eventLog.enabled=true\nspark.databricks.eventLog.listenerClassName=com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.databricks.io.directoryCommit.enableLogicalDelete=false\nspark.databricks.isShieldWorkspace=false\nspark.databricks.managedCatalog.clientClassName=com.databricks.managedcatalog.ManagedCatalogClientImpl\nspark.databricks.metrics.filesystem_io_metrics=true\nspark.databricks.mlflow.autologging.enabled=true\nspark.databricks.overrideDefaultCommitProtocol=org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol\nspark.databricks.passthrough.adls.gen2.tokenProviderClassName=*********(redacted)\nspark.databricks.passthrough.adls.tokenProviderClassName=*********(redacted)\nspark.databricks.passthrough.glue.credentialsProviderFactoryClassName=*********(redacted)\nspark.databricks.passthrough.glue.executorServiceFactoryClassName=*********(redacted)\nspark.databricks.passthrough.oauth.refresher.impl=*********(redacted)\nspark.databricks.passthrough.s3a.threadPoolExecutor.factory.class=com.databricks.backend.daemon.driver.aws.S3APassthroughThreadPoolExecutorFactory\nspark.databricks.passthrough.s3a.tokenProviderClassName=*********(redacted)\nspark.databricks.preemption.enabled=true\nspark.databricks.privateLinkEnabled=false\nspark.databricks.redactor=com.databricks.spark.util.DatabricksSparkLogRedactorProxy\nspark.databricks.repl.enableClassFileCleanup=true\nspark.databricks.secret.envVar.keys.toRedact=*********(redacted)\nspark.databricks.secret.sparkConf.keys.toRedact=*********(redacted)\nspark.databricks.service.dbutils.repl.backend=com.databricks.dbconnect.ReplDBUtils\nspark.databricks.service.dbutils.server.backend=com.databricks.dbconnect.SparkServerDBUtils\nspark.databricks.session.share=false\nspark.databricks.sparkContextId=153907360229993971\nspark.databricks.sql.configMapperClass=com.databricks.dbsql.config.SqlConfigMapperBridge\nspark.databricks.tahoe.logStore.aws.class=com.databricks.tahoe.store.MultiClusterLogStore\nspark.databricks.tahoe.logStore.azure.class=com.databricks.tahoe.store.AzureLogStore\nspark.databricks.tahoe.logStore.class=com.databricks.tahoe.store.DelegatingLogStore\nspark.databricks.tahoe.logStore.gcp.class=com.databricks.tahoe.store.GCPLogStore\nspark.databricks.telemetry.prometheus.samplingRate=100\nspark.databricks.unityCatalog.credentialManager.apiTokenProviderClassName=*********(redacted)\nspark.databricks.unityCatalog.credentialManager.tokenRefreshEnabled=*********(redacted)\nspark.databricks.unityCatalog.enabled=true\nspark.databricks.unityCatalog.enforce.permissions=false\nspark.databricks.unityCatalog.hms.federation.enabled=false\nspark.databricks.unityCatalog.lakehouseFederation.writes.enabled=false\nspark.databricks.unityCatalog.legacy.enableCrossScopeCredCache=true\nspark.databricks.unityCatalog.queryFederation.enabled=true\nspark.databricks.unityCatalog.volumes.enabled=true\nspark.databricks.unityCatalog.volumes.fuse.server.enabled=true\nspark.databricks.workerNodeTypeId=r5d.large\nspark.databricks.workspaceUrl=*********(redacted)\nspark.databricks.wsfs.workspacePrivatePreview=true\nspark.databricks.wsfsPublicPreview=true\nspark.delta.sharing.profile.provider.class=*********(redacted)\nspark.driver.allowMultipleContexts=false\nspark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\nspark.driver.host=10.89.232.166\nspark.driver.maxResultSize=4g\nspark.driver.port=40595\nspark.driver.tempDirectory=/local_disk0/tmp\nspark.eventLog.enabled=false\nspark.executor.extraClassPath=/databricks/spark/dbconf/log4j/executor:/databricks/spark/dbconf/jets3t/:/databricks/spark/dbconf/hadoop:/databricks/hive/conf:/databricks/jars/*\nspark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED --add-opens=java.management/sun.management=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Djava.io.tmpdir=/local_disk0/tmp -XX:ReservedCodeCacheSize=512m -XX:+UseCodeCacheFlushing -XX:PerMethodRecompilationCutoff=-1 -XX:PerBytecodeRecompilationCutoff=-1 -Djava.security.properties=/databricks/spark/dbconf/java/extra.security -XX:-UseContainerSupport -XX:+PrintFlagsFinal -verbose:gc -Xss4m -Djava.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni -Djavax.xml.datatype.DatatypeFactory=com.sun.org.apache.xerces.internal.jaxp.datatype.DatatypeFactoryImpl -Djavax.xml.parsers.DocumentBuilderFactory=com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderFactoryImpl -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -Djavax.xml.validation.SchemaFactory:http://www.w3.org/2001/XMLSchema=com.sun.org.apache.xerces.internal.jaxp.validation.XMLSchemaFactory -Dorg.xml.sax.driver=com.sun.org.apache.xerces.internal.parsers.SAXParser -Dorg.w3c.dom.DOMImplementationSourceList=com.sun.org.apache.xerces.internal.dom.DOMXSImplementationSourceImpl -Djavax.net.ssl.sessionCacheSize=10000 -Dscala.reflect.runtime.disable.typetag.cache=true -Dcom.google.cloud.spark.bigquery.repackaged.io.netty.tryReflectionSetAccessible=true -Dio.netty.tryReflectionSetAccessible=true -Dlog4j2.formatMsgNoLookups=true -Ddatabricks.serviceName=spark-executor-1\nspark.executor.id=driver\nspark.executor.memory=2218m\nspark.executor.tempDirectory=/local_disk0/tmp\nspark.extraListeners=com.databricks.backend.daemon.driver.DBCEventLoggingListener\nspark.files.fetchFailure.unRegisterOutputOnHost=true\nspark.files.overwrite=true\nspark.files.useFetchCache=false\nspark.hadoop.databricks.dbfs.client.version=v2\nspark.hadoop.databricks.fs.perfMetrics.enable=true\nspark.hadoop.databricks.loki.fileStatusCache.enabled=true\nspark.hadoop.databricks.loki.fileSystemCache.enabled=true\nspark.hadoop.databricks.s3.create.deleteUnnecessaryFakeDirectories=false\nspark.hadoop.databricks.s3.verifyBucketExists.enabled=false\nspark.hadoop.databricks.s3commit.client.sslTrustAll=false\nspark.hadoop.fs.AbstractFileSystem.gs.impl=shaded.databricks.com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nspark.hadoop.fs.abfs.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.abfs.impl.disable.cache=true\nspark.hadoop.fs.abfss.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.abfss.impl.disable.cache=true\nspark.hadoop.fs.adl.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.adl.impl.disable.cache=true\nspark.hadoop.fs.azure.authorization.caching.enable=false\nspark.hadoop.fs.azure.cache.invalidator.type=com.databricks.encryption.utils.CacheInvalidatorImpl\nspark.hadoop.fs.azure.skip.metrics=true\nspark.hadoop.fs.azure.user.agent.prefix=*********(redacted)\nspark.hadoop.fs.cpfs-abfss.impl=*********(redacted)\nspark.hadoop.fs.cpfs-abfss.impl.disable.cache=true\nspark.hadoop.fs.cpfs-adl.impl=*********(redacted)\nspark.hadoop.fs.cpfs-adl.impl.disable.cache=true\nspark.hadoop.fs.cpfs-s3.impl=*********(redacted)\nspark.hadoop.fs.cpfs-s3a.impl=*********(redacted)\nspark.hadoop.fs.cpfs-s3n.impl=*********(redacted)\nspark.hadoop.fs.dbfs.impl=com.databricks.backend.daemon.data.client.DbfsHadoop3\nspark.hadoop.fs.dbfsartifacts.impl=com.databricks.backend.daemon.data.client.DBFSV1\nspark.hadoop.fs.fcfs-abfs.impl=*********(redacted)\nspark.hadoop.fs.fcfs-abfs.impl.disable.cache=true\nspark.hadoop.fs.fcfs-abfss.impl=*********(redacted)\nspark.hadoop.fs.fcfs-abfss.impl.disable.cache=true\nspark.hadoop.fs.fcfs-s3.impl=*********(redacted)\nspark.hadoop.fs.fcfs-s3.impl.disable.cache=true\nspark.hadoop.fs.fcfs-s3a.impl=*********(redacted)\nspark.hadoop.fs.fcfs-s3a.impl.disable.cache=true\nspark.hadoop.fs.fcfs-s3n.impl=*********(redacted)\nspark.hadoop.fs.fcfs-s3n.impl.disable.cache=true\nspark.hadoop.fs.fcfs-wasb.impl=*********(redacted)\nspark.hadoop.fs.fcfs-wasb.impl.disable.cache=true\nspark.hadoop.fs.fcfs-wasbs.impl=*********(redacted)\nspark.hadoop.fs.fcfs-wasbs.impl.disable.cache=true\nspark.hadoop.fs.file.impl=com.databricks.backend.daemon.driver.WorkspaceLocalFileSystem\nspark.hadoop.fs.gs.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.gs.impl.disable.cache=true\nspark.hadoop.fs.gs.outputstream.upload.chunk.size=16777216\nspark.hadoop.fs.idbfs.impl=com.databricks.io.idbfs.IdbfsFileSystem\nspark.hadoop.fs.mlflowdbfs.impl=com.databricks.mlflowdbfs.MlflowdbfsFileSystem\nspark.hadoop.fs.s3.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3.impl.disable.cache=true\nspark.hadoop.fs.s3a.assumed.role.credentials.provider=*********(redacted)\nspark.hadoop.fs.s3a.attempts.maximum=10\nspark.hadoop.fs.s3a.block.size=67108864\nspark.hadoop.fs.s3a.connection.maximum=200\nspark.hadoop.fs.s3a.connection.timeout=50000\nspark.hadoop.fs.s3a.fast.upload=true\nspark.hadoop.fs.s3a.fast.upload.active.blocks=32\nspark.hadoop.fs.s3a.fast.upload.default=true\nspark.hadoop.fs.s3a.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3a.impl.disable.cache=true\nspark.hadoop.fs.s3a.max.total.tasks=1000\nspark.hadoop.fs.s3a.multipart.size=10485760\nspark.hadoop.fs.s3a.multipart.threshold=104857600\nspark.hadoop.fs.s3a.retry.interval=250ms\nspark.hadoop.fs.s3a.retry.limit=6\nspark.hadoop.fs.s3a.retry.throttle.interval=500ms\nspark.hadoop.fs.s3a.threads.max=136\nspark.hadoop.fs.s3n.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.s3n.impl.disable.cache=true\nspark.hadoop.fs.stage.impl=com.databricks.backend.daemon.driver.managedcatalog.PersonalStagingFileSystem\nspark.hadoop.fs.stage.impl.disable.cache=true\nspark.hadoop.fs.wasb.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.wasb.impl.disable.cache=true\nspark.hadoop.fs.wasbs.impl=com.databricks.common.filesystem.LokiFileSystem\nspark.hadoop.fs.wasbs.impl.disable.cache=true\nspark.hadoop.hive.hmshandler.retry.attempts=10\nspark.hadoop.hive.hmshandler.retry.interval=2000\nspark.hadoop.hive.server2.enable.doAs=false\nspark.hadoop.hive.server2.idle.operation.timeout=7200000\nspark.hadoop.hive.server2.idle.session.timeout=900000\nspark.hadoop.hive.server2.keystore.password=*********(redacted)\nspark.hadoop.hive.server2.keystore.path=/databricks/keys/jetty-ssl-driver-keystore.jks\nspark.hadoop.hive.server2.session.check.interval=60000\nspark.hadoop.hive.server2.thrift.http.cookie.auth.enabled=false\nspark.hadoop.hive.server2.thrift.http.port=10000\nspark.hadoop.hive.server2.transport.mode=http\nspark.hadoop.hive.server2.use.SSL=true\nspark.hadoop.hive.warehouse.subdir.inherit.perms=false\nspark.hadoop.mapred.output.committer.class=com.databricks.backend.daemon.data.client.DirectOutputCommitter\nspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\nspark.hadoop.parquet.abfs.readahead.optimization.enabled=true\nspark.hadoop.parquet.block.size.row.check.max=10\nspark.hadoop.parquet.block.size.row.check.min=10\nspark.hadoop.parquet.filter.columnindex.enabled=false\nspark.hadoop.parquet.memory.pool.ratio=0.5\nspark.hadoop.parquet.page.metadata.validation.enabled=true\nspark.hadoop.parquet.page.size.check.estimate=false\nspark.hadoop.parquet.page.verify-checksum.enabled=true\nspark.hadoop.parquet.page.write-checksum.enabled=true\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.enabled=false\nspark.hadoop.spark.databricks.io.parquet.verifyChecksumOnWrite.throwsException=false\nspark.hadoop.spark.databricks.metrics.filesystem_metrics=true\nspark.hadoop.spark.driverproxy.customHeadersToProperties=*********(redacted)\nspark.hadoop.spark.hadoop.aws.glue.cache.db.size=1000\nspark.hadoop.spark.hadoop.aws.glue.cache.db.ttl-mins=30\nspark.hadoop.spark.hadoop.aws.glue.cache.table.size=1000\nspark.hadoop.spark.hadoop.aws.glue.cache.table.ttl-mins=30\nspark.hadoop.spark.sql.parquet.output.committer.class=org.apache.spark.sql.parquet.DirectParquetOutputCommitter\nspark.hadoop.spark.sql.sources.outputCommitterClass=com.databricks.backend.daemon.data.client.MapReduceDirectOutputCommitter\nspark.home=/databricks/spark\nspark.logConf=true\nspark.master=spark://10.89.232.166:7077\nspark.memory.offHeap.enabled=true\nspark.memory.offHeap.size=6978273280\nspark.metrics.conf=/databricks/spark/conf/metrics.properties\nspark.r.backendConnectionTimeout=604800\nspark.r.numRBackendThreads=1\nspark.rdd.compress=true\nspark.repl.class.outputDir=/local_disk0/tmp/repl/spark-153907360229993971-44272e49-f0a6-400a-be29-d8b892db0d30\nspark.rpc.message.maxSize=256\nspark.scheduler.listenerbus.eventqueue.capacity=20000\nspark.scheduler.mode=FAIR\nspark.serializer.objectStreamReset=100\nspark.shuffle.manager=SORT\nspark.shuffle.memoryFraction=0.2\nspark.shuffle.reduceLocality.enabled=false\nspark.shuffle.service.enabled=true\nspark.shuffle.service.port=4048\nspark.sparklyr-backend.threads=1\nspark.sparkr.use.daemon=false\nspark.speculation=false\nspark.speculation.multiplier=3\nspark.speculation.quantile=0.9\nspark.sql.allowMultipleContexts=false\nspark.sql.hive.convertCTAS=true\nspark.sql.hive.convertMetastoreParquet=true\nspark.sql.hive.metastore.jars=/databricks/databricks-hive/*\nspark.sql.hive.metastore.sharedPrefixes=org.mariadb.jdbc,com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,microsoft.sql.DateTimeOffset,microsoft.sql.Types,com.databricks,com.codahale,com.fasterxml.jackson,shaded.databricks\nspark.sql.hive.metastore.version=0.13.0\nspark.sql.legacy.createHiveTableByDefault=false\nspark.sql.parquet.cacheMetadata=true\nspark.sql.parquet.compression.codec=snappy\nspark.sql.sources.commitProtocolClass=com.databricks.sql.transaction.directory.DirectoryAtomicCommitProtocol\nspark.sql.sources.default=delta\nspark.sql.streaming.checkpointFileManagerClass=com.databricks.spark.sql.streaming.DatabricksCheckpointFileManager\nspark.sql.streaming.stopTimeout=15s\nspark.sql.warehouse.dir=*********(redacted)\nspark.storage.blockManagerTimeoutIntervalMs=300000\nspark.storage.memoryFraction=0.5\nspark.streaming.driver.writeAheadLog.allowBatching=true\nspark.streaming.driver.writeAheadLog.closeFileAfterWrite=true\nspark.task.reaper.enabled=true\nspark.task.reaper.killTimeout=60s\nspark.ui.port=40001\nspark.ui.prometheus.enabled=true\nspark.worker.aioaLazyConfig.dbfsReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosDbfsReadinessCheckClient\nspark.worker.aioaLazyConfig.iamReadinessCheckClientClass=com.databricks.backend.daemon.driver.NephosIamRoleCheckClient\nspark.worker.cleanup.enabled=false"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "WARN",
      "message": "MetricsSystem: Using default name SparkStatusTracker for source because neither spark.metrics.namespace nor spark.app.id is set."
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:30",
      "level": "INFO",
      "message": "log: Logging initialized @32615ms to org.eclipse.jetty.util.log.Slf4jLog"
    },
    {
      "timestamp": "24/07/02 20:00:31",
      "level": "INFO",
      "message": "JettyUtils: Start Jetty 10.89.232.166:40001 for SparkUI"
    },
    {
      "timestamp": "24/07/02 20:00:31",
      "level": "INFO",
      "message": "Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.21+9-LTS"
    },
    {
      "timestamp": "24/07/02 20:00:31",
      "level": "INFO",
      "message": "Server: Started @33108ms"
    },
    {
      "timestamp": "24/07/02 20:00:31",
      "level": "INFO",
      "message": "AbstractConnector: Started ServerConnector@70de1e65{HTTP/1.1, (http/1.1)}{10.89.232.166:40001}"
    },
    {
      "timestamp": "24/07/02 20:00:31",
      "level": "INFO",
      "message": "Utils: Successfully started service 'SparkUI' on port 40001."
    },
    {
      "timestamp": "24/07/02 20:00:31",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@43c16e17{/,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:33",
      "level": "INFO",
      "message": "DriverPluginContainer: Initialized driver component for plugin org.apache.spark.sql.connect.SparkConnectPlugin."
    },
    {
      "timestamp": "24/07/02 20:00:33",
      "level": "INFO",
      "message": "DLTDebugger: Registered DLTDebuggerEndpoint at endpoint dlt-debugger"
    },
    {
      "timestamp": "24/07/02 20:00:34",
      "level": "INFO",
      "message": "DriverPluginContainer: Initialized driver component for plugin org.apache.spark.debugger.DLTDebuggerSparkPlugin."
    },
    {
      "timestamp": "24/07/02 20:00:34",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Fair scheduler configuration not found, created default pool: default, schedulingMode: FAIR, minShare: 0, weight: 1"
    },
    {
      "timestamp": "24/07/02 20:00:35",
      "level": "INFO",
      "message": "StandaloneAppClient$ClientEndpoint: Connecting to master spark://10.89.232.166:7077..."
    },
    {
      "timestamp": "24/07/02 20:00:35",
      "level": "INFO",
      "message": "TransportClientFactory: Successfully created connection to /10.89.232.166:7077 after 353 ms (0 ms spent in bootstraps)"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Task preemption enabled."
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "StandaloneAppClient$ClientEndpoint: Executor added: app-20240702200036-0000/0 on worker-20240702200004-10.89.238.116-33589 (10.89.238.116:33589) with 2 core(s)"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "StandaloneSchedulerBackend: Granted executor ID app-20240702200036-0000/0 on hostPort 10.89.238.116:33589 with 2 core(s), 2.2 GiB RAM"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45711."
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "NettyBlockTransferService: Server created on 10.89.232.166:45711"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "BlockManager: external shuffle service port = 4048"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.89.232.166, 45711, None)"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "BlockManagerMasterEndpoint: Registering block manager 10.89.232.166:45711 with 10.9 GiB RAM, BlockManagerId(driver, 10.89.232.166, 45711, None)"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.89.232.166, 45711, None)"
    },
    {
      "timestamp": "24/07/02 20:00:36",
      "level": "INFO",
      "message": "BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.89.232.166, 45711, None)"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "DBCEventLoggingListener: Initializing DBCEventLoggingListener (compressionEnabled=true)"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "DBCEventLoggingListener: Logging events to eventlogs/153907360229993971/eventlog"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "SparkContext: Registered listener com.databricks.backend.daemon.driver.DBCEventLoggingListener"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "DatabricksILoop$: Finished creating throwaway interpreter"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "StandaloneAppClient$ClientEndpoint: Executor updated: app-20240702200036-0000/0 is now RUNNING"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.21+9-LTS"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@e3af46a{/,null,AVAILABLE}"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "SslContextFactory: x509=X509@15c33dae(1,h=[nvirginia-prod.workers.prod.ns.databricks.com],a=[],w=[]) for Server@398e457a[provider=null,keyStore=file:///databricks/keys/jetty_ssl_driver_keystore.jks,trustStore=file:///databricks/keys/jetty_ssl_driver_keystore.jks]"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "AbstractConnector: Started ServerConnector@75753f3d{SSL, (ssl, http/1.1)}{0.0.0.0:1023}"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "Server: Started @39749ms"
    },
    {
      "timestamp": "24/07/02 20:00:37",
      "level": "INFO",
      "message": "FuseDaemonServer: FuseDaemonServer started on 1023 with endpoint: '/get-unity-token'."
    },
    {
      "timestamp": "24/07/02 20:00:38",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:38",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3a. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:38",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme s3n. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:38",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfs. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:38",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme abfss. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:38",
      "level": "INFO",
      "message": "SparkHadoopUtil: Installing CredentialsScopeFilesystem for scheme gs. Previous value: com.databricks.common.filesystem.LokiFileSystem"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Stopped o.e.j.s.ServletContextHandler@43c16e17{/,null,STOPPED,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@7de752dd{/jobs,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@201bc8f0{/jobs/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@3a8a2089{/jobs/job,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@4bd47b75{/jobs/job/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@2579cc85{/stages,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@47a1b2f7{/stages/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@98be09f{/stages/stage,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@222456bc{/stages/stage/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@7c9745ae{/stages/pool,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@5b481d77{/stages/pool/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@411d3b5e{/storage,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@48667fd3{/storage/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@6ac3ceda{/storage/rdd,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@3145c33b{/storage/rdd/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@61dc8a64{/environment,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@16c230c{/environment/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@789f27e5{/executors,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@7dfe23c2{/executors/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@29f32a41{/executors/threadDump,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@6801b4a4{/executors/threadDump/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@647b7f63{/executors/heapHistogram,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@e06d647{/executors/heapHistogram/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@326471ab{/executors/heapHistogram,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@5e232d07{/executors/heapHistogram/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@1e69a310{/static,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@2e13c72b{/,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@14cd3858{/api,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@29810fa0{/metrics,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@311d2611{/jobs/job/kill,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@5b11b6c8{/stages/stage/kill,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@797ec046{/metrics/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "DatabricksILoop$: Successfully registered spark metrics in Prometheus registry"
    },
    {
      "timestamp": "24/07/02 20:00:39",
      "level": "INFO",
      "message": "DatabricksILoop$: Successfully initialized SparkContext"
    },
    {
      "timestamp": "24/07/02 20:00:40",
      "level": "INFO",
      "message": "SharedState: Scheduler stats enabled."
    },
    {
      "timestamp": "24/07/02 20:00:40",
      "level": "INFO",
      "message": "SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir."
    },
    {
      "timestamp": "24/07/02 20:00:40",
      "level": "INFO",
      "message": "SharedState: Warehouse path is 'dbfs:/user/hive/warehouse'."
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@175f1ff5{/storage/iocache,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@98a954a{/storage/iocache/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@18681121{/SQL,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@78ffe38{/SQL/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@4e24b201{/SQL/execution,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@787aa9f1{/SQL/execution/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@61902ae5{/static/sql,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:00:41",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:00:45",
      "level": "INFO",
      "message": "DriverConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:45",
      "level": "WARN",
      "message": "DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:45",
      "level": "INFO",
      "message": "DriverConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:45",
      "level": "WARN",
      "message": "DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:46",
      "level": "INFO",
      "message": "Utils: resolved command to be run: WrappedArray(getconf, PAGESIZE)"
    },
    {
      "timestamp": "24/07/02 20:00:46",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 228.127953 ms."
    },
    {
      "timestamp": "24/07/02 20:00:47",
      "level": "INFO",
      "message": "DatabricksMountsStore: Mount store initialization: Attempting to get the list of mounts from metadata manager of DBFS"
    },
    {
      "timestamp": "24/07/02 20:00:47",
      "level": "INFO",
      "message": "log: Logging initialized @49430ms to shaded.v9_4.org.eclipse.jetty.util.log.Slf4jLog"
    },
    {
      "timestamp": "24/07/02 20:00:47",
      "level": "INFO",
      "message": "DynamicRpcConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:47",
      "level": "WARN",
      "message": "DynamicRpcConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:51",
      "level": "INFO",
      "message": "StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered ()executor NettyRpcEndpointRef(spark-client://Executor) (10.89.238.116:39804) with ID 0, ResourceProfileId 0"
    },
    {
      "timestamp": "24/07/02 20:00:52",
      "level": "INFO",
      "message": "BlockManagerMasterEndpoint: Registering block manager 10.89.238.116:32951 with 7.6 GiB RAM, BlockManagerId(0, 10.89.238.116, 32951, None)"
    },
    {
      "timestamp": "24/07/02 20:00:52",
      "level": "INFO",
      "message": "DatabricksMountsStore: Mount store initialization: Received a list of 10 mounts accessible from metadata manager of DBFS"
    },
    {
      "timestamp": "24/07/02 20:00:52",
      "level": "INFO",
      "message": "DatabricksMountsStore: Updated mounts cache. Changes: List((+,DbfsMountPoint(s3a://databricks-datasets-virginia/, /databricks-datasets)), (+,DbfsMountPoint(uc-volumes:/Volumes, /Volumes)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-tracking)), (+,DbfsMountPoint(s3a://databricks-workspace-stack-53375-bucket/ephemeral/nvirginia-prod/42883594658554, /databricks-results)), (+,DbfsMountPoint(s3a://big-data-education, /mnt/big_data_edu)), (+,DbfsMountPoint(unsupported-access-mechanism-for-path--use-mlflow-client:/, /databricks/mlflow-registry)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /Volume)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /volumes)), (+,DbfsMountPoint(s3a://databricks-workspace-stack-53375-bucket/nvirginia-prod/42883594658554, /)), (+,DbfsMountPoint(dbfs-reserved-path:/uc-volumes-reserved, /volume)))"
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "HadoopFSUtil$: Installing the old filesystem implementation for scheme s3n. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem."
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "HadoopFSUtil$: Installing the old filesystem implementation for scheme s3a. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem."
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "HadoopFSUtil$: Installing the old filesystem implementation for scheme abfss. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem."
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "HadoopFSUtil$: Installing the old filesystem implementation for scheme gs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem."
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "HadoopFSUtil$: Installing the old filesystem implementation for scheme s3. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem."
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "HadoopFSUtil$: Installing the old filesystem implementation for scheme abfs. Current value: com.databricks.sql.acl.fs.CredentialScopeFileSystem. Old value to be restored: com.databricks.common.filesystem.LokiFileSystem."
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "DatabricksFileSystemV2Factory: Creating S3A file system for s3a://databricks-workspace-stack-53375-bucket"
    },
    {
      "timestamp": "24/07/02 20:00:53",
      "level": "INFO",
      "message": "deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DbfsHadoop3: Initialized DBFS with DBFSV2 as the delegate."
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DatabricksILoop$: Trying to load dynamic config on startup: /databricks/driver/conf/dynamicSparkConfig.conf"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DatabricksILoop$: Read the dynamic config: Vector(SaferConf(spark.databricks.dynamicConf.demoFlag,0,1714612443,240305032914554,1)) from /databricks/driver/conf/dynamicSparkConfig.conf, version 1719950291714"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "LibraryResolutionManager: Preferred maven central mirror is configured to https://maven-central.storage-download.googleapis.com/maven2/"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "WARN",
      "message": "OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-burst. Using default: 100000000000"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "WARN",
      "message": "OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-steady-rate. Using default: 6000000000"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "WARN",
      "message": "OutgoingDirectNotebookBufferRateLimiter$: No value specified for db-outgoing-buffer-throttler-warning-interval-sec. Using default: 60"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@fc41a1d{/StreamingQuery,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@34420dd3{/StreamingQuery/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@5c806f1d{/StreamingQuery/statistics,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@42f16e74{/StreamingQuery/statistics/json,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "ContextHandler: Started o.e.j.s.ServletContextHandler@699b37d4{/static/sql,null,AVAILABLE,@Spark}"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "JettyServer$: Creating thread pool with name ..."
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "JettyServer$: Thread pool created"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "JettyServer$: Creating thread pool with name ..."
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "JettyServer$: Thread pool created"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DriverDaemon: Starting driver daemon..."
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.password"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.databricks.io.directoryCommit.enableLogicalDelete"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "WARN",
      "message": "SparkConfUtils$: Setting the same key twice for spark.hadoop.hive.server2.keystore.path"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "SparkConfUtils$: Customize spark config according to file /tmp/custom-spark.conf"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DriverDaemon$: Attempting to run: 'set up ttyd daemon'"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DriverDaemon$: Attempting to run: 'Configuring RStudio daemon'"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DriverDaemon$: Attempting to run: 'set up git-agent daemon'"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DriverDaemon$: Attempting to run: 'Create home directory for libraries user'"
    },
    {
      "timestamp": "24/07/02 20:00:54",
      "level": "INFO",
      "message": "DriverDaemon$: Resetting the default python executable"
    },
    {
      "timestamp": "24/07/02 20:00:55",
      "level": "INFO",
      "message": "Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/cluster_libraries/python, -p, /databricks/python/bin/python, --no-download, --no-setuptools, --no-wheel)"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/cluster_libraries/python"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "Utils: resolved command to be run: List(/databricks/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, from sysconfig import get_path; print(get_path('purelib')))"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages/sites.pth"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "ClusterWidePythonEnvManager: Registered /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@2d1e3187"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "DriverDaemon$: Attempting to run: 'Update root virtualenv'"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "DriverDaemon$: Finished updating /etc/environment"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "DriverDaemon$$anon$1: Thread to send message is ready"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "NetstatUtil$: Running netstat -lnpt"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "NetstatUtil$: netstat -lnpt\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 0.0.0.0:7681            0.0.0.0:*               LISTEN      1121/ttyd           \ntcp        0      0 0.0.0.0:2812            0.0.0.0:*               LISTEN      80/monit            \ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      55/systemd-resolved \ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      81/sshd: /usr/sbin/ \ntcp6       0      0 :::15002                :::*                    LISTEN      853/java            \ntcp6       0      0 10.89.232.166:45711     :::*                    LISTEN      853/java            \ntcp6       0      0 :::7071                 :::*                    LISTEN      455/java            \ntcp6       0      0 10.89.232.166:40595     :::*                    LISTEN      853/java            \ntcp6       0      0 :::6060                 :::*                    LISTEN      455/java            \ntcp6       0      0 10.89.232.166:40000     :::*                    LISTEN      593/java            \ntcp6       0      0 10.89.232.166:40001     :::*                    LISTEN      853/java            \ntcp6       0      0 10.89.232.166:7077      :::*                    LISTEN      593/java            \ntcp6       0      0 :::2812                 :::*                    LISTEN      80/monit            \ntcp6       0      0 :::1015                 :::*                    LISTEN      281/goofys-dbr      \ntcp6       0      0 :::1017                 :::*                    LISTEN      273/wsfs            \ntcp6       0      0 :::1021                 :::*                    LISTEN      273/wsfs            \ntcp6       0      0 :::1023                 :::*                    LISTEN      853/java            \ntcp6       0      0 :::22                   :::*                    LISTEN      81/sshd: /usr/sbin/ \n"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.21+9-LTS"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "AbstractConnector: Started ServerConnector@7f914800{HTTP/1.1, (http/1.1)}{0.0.0.0:6061}"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "Server: Started @58529ms"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "Server: jetty-9.4.51.v20230217; built: 2023-02-17T08:19:37.309Z; git: b45c405e4544384de066f814ed42ae3dceacdd49; jvm 11.0.21+9-LTS"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "SslContextFactory: x509=X509@25df373d(1,h=[nvirginia-prod.workers.prod.ns.databricks.com],a=[],w=[]) for Server@36806af3[provider=null,keyStore=null,trustStore=null]"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "WARN",
      "message": "config: Weak cipher suite TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA enabled for Server@36806af3[provider=null,keyStore=null,trustStore=null]"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "WARN",
      "message": "config: Weak cipher suite TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA enabled for Server@36806af3[provider=null,keyStore=null,trustStore=null]"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "AbstractConnector: Started ServerConnector@45895918{SSL, (ssl, http/1.1)}{0.0.0.0:6062}"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "Server: Started @58603ms"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "DriverDaemon: Started comm channel server"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "DriverDaemon: Driver daemon started."
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "INFO",
      "message": "DynamicInfoServiceConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:00:56",
      "level": "WARN",
      "message": "DynamicInfoServiceConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:00:57",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:00:57",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Loading the root classloader"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Starting sql repl ReplId-3a704-30266-473ea-0"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Starting sql repl ReplId-570cb-5b182-3a854-9"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Starting sql repl ReplId-417fc-20f5a-c320a-2"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Starting sql repl ReplId-761a3-29b0d-aa8e8-8"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Starting sql repl ReplId-41a90-92107-3e084-a"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "SQLDriverWrapper: setupRepl:ReplId-41a90-92107-3e084-a: finished to load"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "SQLDriverWrapper: setupRepl:ReplId-417fc-20f5a-c320a-2: finished to load"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "SQLDriverWrapper: setupRepl:ReplId-3a704-30266-473ea-0: finished to load"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "SQLDriverWrapper: setupRepl:ReplId-761a3-29b0d-aa8e8-8: finished to load"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "SQLDriverWrapper: setupRepl:ReplId-570cb-5b182-3a854-9: finished to load"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "DriverCorral: Starting r repl ReplId-63968-7d67f-b17ab-e"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "ROutputStreamHandler: Connection succeeded on port 33899"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "ROutputStreamHandler: Connection succeeded on port 42577"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "RDriverLocal: 1. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: object created with for ReplId-63968-7d67f-b17ab-e."
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "RDriverLocal: 2. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: initializing ..."
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "RDriverLocal: 3. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: started RBackend thread on port 46313"
    },
    {
      "timestamp": "24/07/02 20:00:58",
      "level": "INFO",
      "message": "RDriverLocal: 4. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: waiting for SparkR to be installed ..."
    },
    {
      "timestamp": "24/07/02 20:00:59",
      "level": "WARN",
      "message": "DriverDaemon: ShouldUseAutoscalingInfo exception thrown, not logging stack trace. This is used for control flow and is ok to ignore"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "DriverCorral: [Thread 150] AttachLibraries - candidate libraries: List(com.databricks.libraries.JavaJarId@ccbf1993)"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "DriverCorral: [Thread 150] AttachLibraries - new libraries to install (including resolved dependencies): List(com.databricks.libraries.JavaJarId@ccbf1993)"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "SharedDriverContext: [Thread 150] attachLibrariesToSpark com.databricks.libraries.JavaJarId@ccbf1993"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "SharedDriverContext: Attaching lib: dbfs:/Volumes/sales/dims/ingestion_zone/demo/spark-transformations-0.1-jar-with-dependencies.jar to Spark"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "DriverConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "WARN",
      "message": "DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "LibraryDownloadManager: Downloading a library that was not in the cache: com.databricks.libraries.JavaJarId@ccbf1993"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "LibraryDownloadManager: Attempt 1: wait until library com.databricks.libraries.JavaJarId@ccbf1993 is downloaded"
    },
    {
      "timestamp": "24/07/02 20:01:10",
      "level": "INFO",
      "message": "LibraryDownloadManager: Preparing to download library file from UC Volume path: dbfs:/Volumes/sales/dims/ingestion_zone/demo/spark-transformations-0.1-jar-with-dependencies.jar"
    },
    {
      "timestamp": "24/07/02 20:01:12",
      "level": "WARN",
      "message": "DriverDaemon: ShouldUseAutoscalingInfo exception thrown, not logging stack trace. This is used for control flow and is ok to ignore"
    },
    {
      "timestamp": "24/07/02 20:01:12",
      "level": "INFO",
      "message": "DriverConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:01:12",
      "level": "WARN",
      "message": "DriverConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:01:13",
      "level": "INFO",
      "message": "AWSCredentialProviderList:V3: Using credentials for bucket databricks-workspace-stack-53375-metastore-bucket-vf from com.databricks.backend.daemon.driver.credentials.CredentialScopeS3TokenProvider"
    },
    {
      "timestamp": "24/07/02 20:01:13",
      "level": "WARN",
      "message": "DatabricksS3LoggingUtils$:V3: Error calling HeadBucket: 403. This is typically used to infer the region of the bucket and is harmless. Details: Request ID: null, Extended Request ID: null, Cloud Provider: AWS, Instance ID: i-0efa059c748dd4582 Response Headers: Map(Server -> AmazonS3, x-amz-id-2 -> 649prw7v92LMai9RfmSE0NgFSfC28XzoMK0wp+W/4MuvhWoGwNRbK0aEP27yR1zihK5qjH7F9Xs=, x-amz-bucket-region -> us-east-1, Content-Type -> application/xml, x-amz-request-id -> TDFW0J7BS8Y5YY04, Date -> Tue, 02 Jul 2024 20:01:12 GMT)"
    },
    {
      "timestamp": "24/07/02 20:01:17",
      "level": "INFO",
      "message": "RDriverLocal$: SparkR installation completed."
    },
    {
      "timestamp": "24/07/02 20:01:17",
      "level": "INFO",
      "message": "RDriverLocal: 5. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: launching R process ..."
    },
    {
      "timestamp": "24/07/02 20:01:17",
      "level": "INFO",
      "message": "RDriverLocal: 6. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: cgroup isolation disabled, not placing R process in REPL cgroup."
    },
    {
      "timestamp": "24/07/02 20:01:17",
      "level": "INFO",
      "message": "RDriverLocal: 7. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: starting R process on port 1100 (attempt 1) ..."
    },
    {
      "timestamp": "24/07/02 20:01:17",
      "level": "INFO",
      "message": "RDriverLocal$: Debugging command for R process builder: SIMBASPARKINI=/etc/simba.sparkodbc.ini R_LIBS=/local_disk0/.ephemeral_nfs/envs/rEnv-acc7f8f0-3a0c-4076-9651-e2dae4ebf703:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r LD_LIBRARY_PATH=/opt/simba/sparkodbc/lib/64/ SPARKR_BACKEND_CONNECTION_TIMEOUT=604800 DB_STREAM_BEACON_STRING_START=DATABRICKS_STREAM_START-ReplId-63968-7d67f-b17ab-e DB_STDOUT_STREAM_PORT=33899 SPARKR_BACKEND_AUTH_SECRET=b1299c2fa0d09ad2a79c81030f375a8658cb7ecbeb67d3432f78c45c37f8851b DB_STREAM_BEACON_STRING_END=DATABRICKS_STREAM_END-ReplId-63968-7d67f-b17ab-e EXISTING_SPARKR_BACKEND_PORT=46313 ODBCINI=/etc/odbc.ini DB_STDERR_STREAM_PORT=42577 /bin/bash /local_disk0/tmp/_startR.sh2534755526476531516resource.r /local_disk0/tmp/_rServeScript.r6295533303008663552resource.r 1100 None"
    },
    {
      "timestamp": "24/07/02 20:01:17",
      "level": "INFO",
      "message": "RDriverLocal: 8. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: setting up BufferedStreamThread with bufferSize: 1000."
    },
    {
      "timestamp": "24/07/02 20:01:19",
      "level": "INFO",
      "message": "RDriverLocal: 9. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: R process started with RServe listening on port 1100."
    },
    {
      "timestamp": "24/07/02 20:01:19",
      "level": "INFO",
      "message": "RDriverLocal: 10. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: starting interpreter to talk to R process ..."
    },
    {
      "timestamp": "24/07/02 20:01:20",
      "level": "WARN",
      "message": "SparkContext: Using an existing SparkContext; some configuration may not take effect."
    },
    {
      "timestamp": "24/07/02 20:01:20",
      "level": "INFO",
      "message": "ROutputStreamHandler: Successfully connected to stdout in the RShell."
    },
    {
      "timestamp": "24/07/02 20:01:20",
      "level": "INFO",
      "message": "ROutputStreamHandler: Successfully connected to stderr in the RShell."
    },
    {
      "timestamp": "24/07/02 20:01:20",
      "level": "INFO",
      "message": "RDriverLocal: 11. RDriverLocal.6e11383b-9d2d-4f14-b66c-7cc4d3398653: R interpreter is connected."
    },
    {
      "timestamp": "24/07/02 20:01:20",
      "level": "INFO",
      "message": "RDriverWrapper: setupRepl:ReplId-63968-7d67f-b17ab-e: finished to load"
    },
    {
      "timestamp": "24/07/02 20:01:24",
      "level": "INFO",
      "message": "LibraryDownloadManager: Downloaded library com.databricks.libraries.JavaJarId@ccbf1993 as local file /local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar in 14155 milliseconds"
    },
    {
      "timestamp": "24/07/02 20:01:24",
      "level": "INFO",
      "message": "SharedDriverContext: Successfully saved library com.databricks.libraries.JavaJarId@ccbf1993 to local file /local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar"
    },
    {
      "timestamp": "24/07/02 20:01:24",
      "level": "INFO",
      "message": "SparkContext: Added file /local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar at spark://10.89.232.166:40595/files/spark_transformations_0_1_jar_with_dependencies.jar with timestamp 1719950484803"
    },
    {
      "timestamp": "24/07/02 20:01:24",
      "level": "INFO",
      "message": "Utils: Copying /local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar to /local_disk0/spark-07bef6d4-1680-44a0-a00f-9a5b60e8ada4/userFiles-a96ea3e9-a4d3-4d59-9344-3e615984f8b2/spark_transformations_0_1_jar_with_dependencies.jar"
    },
    {
      "timestamp": "24/07/02 20:01:25",
      "level": "INFO",
      "message": "SparkContext: Added JAR /local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar at (spark://10.89.232.166:40595/jars/spark_transformations_0_1_jar_with_dependencies.jar,Some(/local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar)) with timestamp 1719950485495"
    },
    {
      "timestamp": "24/07/02 20:01:25",
      "level": "INFO",
      "message": "SharedDriverContext: Successfully attached library dbfs:/Volumes/sales/dims/ingestion_zone/demo/spark-transformations-0.1-jar-with-dependencies.jar to Spark"
    },
    {
      "timestamp": "24/07/02 20:01:25",
      "level": "INFO",
      "message": "LibraryState: [Thread 150] Successfully attached library dbfs:/Volumes/sales/dims/ingestion_zone/demo/spark-transformations-0.1-jar-with-dependencies.jar"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "DriverCorral: Starting scala repl ReplId-7ff5f-999f3-d1c14-8"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "DynamicTracingConf: Configured feature flag data source LaunchDarkly"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "WARN",
      "message": "DynamicTracingConf: REGION environment variable is not defined. getConfForCurrentRegion will always return default value"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "CurrentQueryContext: Thread Thread[WRAPPER-ReplId-7ff5f-999f3-d1c14-8,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.command.AddJarsCommand)."
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Added query with execution ID:0. Current active queries:1"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "AdaptiveParallelism: Updating parallelism using instant cluster load. Old parallelism: 1, Total cores: 2, Current load: 1, Current Avg load: 0, New parallelism: 2"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "LogicalPlanStats: Setting LogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "BaselineLogicalPlanStats: Setting BaselineLogicalPlanStats visitor to com.databricks.sql.optimizer.statsEstimation.DatabricksLogicalPlanStatsVisitor$"
    },
    {
      "timestamp": "24/07/02 20:01:37",
      "level": "INFO",
      "message": "ExperimentalLogicalPlanStats: Setting ExperimentalLogicalPlanStats visitors to List((NonEquiJoinCondition,com.databricks.sql.optimizer.statsEstimation.experiments.NonEquiJoinConditionLogicalPlanStatsVisitor$@6de9dd8e), (WindowStats,com.databricks.sql.optimizer.statsEstimation.experiments.WindowStatsLogicalPlanStatsVisitor$@eacf0c5), (JoinSelectivityOne,com.databricks.sql.optimizer.statsEstimation.experiments.JoinSelectivityOneExperimentalVisitor$@4d0ab7ed), (JoinSelectivityOneGuard,com.databricks.sql.optimizer.statsEstimation.experiments.JoinSelectivityOneGuardExperimentalVisitor$@cc08938), (LimitStats,com.databricks.sql.optimizer.statsEstimation.experiments.LimitStatsLogicalPlanStatsVisitor$@2f663083), (PropagateStatsJoinedExpressions,com.databricks.sql.optimizer.statsEstimation.experiments.PropagateStatsForJoinedExpressionsExperimentalVisitor$@46a5c772))"
    },
    {
      "timestamp": "24/07/02 20:01:38",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:01:40",
      "level": "INFO",
      "message": "NativeLibraryLoader: Loaded library lib-photon-release.so from  /databricks/native/lib-photon-release.so with glog sink /databricks/driver/logs"
    },
    {
      "timestamp": "24/07/02 20:01:41",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "JniApi: Built case mappings for en in 2426.119716 ms"
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "JniApi: Built char info in 36.090503 ms"
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "JniApi: Time to copy 7340032 bytes is 537935 ns"
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "JniApi: Initializing Photon library via JNI with glogSink = /databricks/driver/logs."
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "JniApi: Initialized Photon library in 105.23723299999999 ms."
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "JniApi: Photon properties:\narrow_batch_size: 24\narrow_batch_row_count_offset: 0\narrow_batch_is_compressed_offset: 4\narrow_batch_uncompressed_size_bytes_offset: 8\narrow_batch_compressed_size_bytes_offset: 12\narrow_batch_data_offset: 16\ncolumn_vector_size: 72\ncolumn_vector_data_offset: 0\ncolumn_vector_nulls_offset: 8\ncolumn_vector_has_nulls_offset: 16\ncolumn_vector_offsets_offset: 24\ncolumn_vector_lengths_offset: 32\ncolumn_vector_dict_offset: 40\ncolumn_batch_size: 40\ncolumn_batch_vectors_offset: 0\ncolumn_batch_num_cols_offset: 8\ncolumn_batch_num_rows_offset: 12\ncolumn_batch_active_rows_offset: 16\nvar_len_val_size: 16\nvar_len_val_len_offset: 0\nvar_len_val_data_offset: 8\nactive_rows_data_offset: 0\nactive_rows_size_offset: 8\ncolumn_vector_dict_size_offset: 48\ncolumn_vector_dict_element_size_offset: 52\ncolumn_vector_num_children_offset: 56\narray_val_size: 16\narray_val_len_offset: 0\narray_val_location_offset: 8\ncolumn_vector_children_offset: 64\n"
    },
    {
      "timestamp": "24/07/02 20:01:43",
      "level": "INFO",
      "message": "deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "deprecation: fs.default.name is deprecated. Instead, use fs.defaultFS"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "SecuredHiveExternalCatalog: creating hiveClient from java.lang.Throwable\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:80)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:78)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.maybeSynchronized(HiveExternalCatalog.scala:114)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$withClient$1(HiveExternalCatalog.scala:154)\n\tat com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:397)\n\tat com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:153)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:328)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:304)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:299)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:59)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$resourceLoader$1(HiveSessionStateBuilder.scala:66)\n\tat org.apache.spark.sql.hive.HiveSessionResourceLoader.client$lzycompute(HiveSessionStateBuilder.scala:165)\n\tat org.apache.spark.sql.hive.HiveSessionResourceLoader.client(HiveSessionStateBuilder.scala:165)\n\tat org.apache.spark.sql.hive.HiveSessionResourceLoader.$anonfun$addJar$1(HiveSessionStateBuilder.scala:169)\n\tat org.apache.spark.sql.hive.HiveSessionResourceLoader.$anonfun$addJar$1$adapted(HiveSessionStateBuilder.scala:168)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.hive.HiveSessionResourceLoader.addJar(HiveSessionStateBuilder.scala:168)\n\tat org.apache.spark.sql.execution.command.AddJarsCommand.$anonfun$run$1(resources.scala:38)\n\tat org.apache.spark.sql.execution.command.AddJarsCommand.$anonfun$run$1$adapted(resources.scala:36)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.execution.command.AddJarsCommand.run(resources.scala:36)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:82)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:80)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:79)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:286)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:166)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:286)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:298)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:528)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:225)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:154)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:477)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:285)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:259)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:280)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:265)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:465)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:465)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:339)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:335)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:39)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:441)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:265)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:395)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:265)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:217)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:214)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:261)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1155)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1155)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:112)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$5(SparkSession.scala:928)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:917)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$9(SparkSession.scala:951)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1148)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:951)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:984)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:695)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$6(DriverLocal.scala:489)\n\tat org.apache.spark.SafeAddJarOrFile$.safe(SafeAddJarOrFile.scala:31)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$5(DriverLocal.scala:489)\n\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$1(CheckPermissions.scala:2036)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:129)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2036)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$4(DriverLocal.scala:488)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$3(DriverLocal.scala:481)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$new$2(DriverLocal.scala:480)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat com.databricks.backend.daemon.driver.DriverLocal.<init>(DriverLocal.scala:467)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.<init>(ScalaDriverLocal.scala:54)\n\tat com.databricks.backend.daemon.driver.ScalaDriverWrapper.instantiateDriver(DriverWrapper.scala:975)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.setupRepl(DriverWrapper.scala:414)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:295)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "HiveConf: Found configuration file file:/databricks/hive/conf/hive-site.xml"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "WARN",
      "message": "SQLConf: The SQL config 'spark.sql.hive.convertCTAS' has been deprecated in Spark v3.1 and may be removed in the future. Set 'spark.sql.legacy.createHiveTableByDefault' to false instead."
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "HiveUtils: Initializing HiveMetastoreConnection version 0.13.0 using file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--javax.mail--mail--javax.mail__mail__1.4.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.groovy--groovy-all--org.codehaus.groovy__groovy-all__2.1.6.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-collections--commons-collections--commons-collections__commons-collections__3.2.2.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.reflectasm--reflectasm--com.esotericsoftware.reflectasm__reflectasm__1.07-shaded.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.zookeeper--zookeeper--org.apache.zookeeper__zookeeper__3.4.5.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--net.sf.jpam--jpam--net.sf.jpam__jpam__1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.google.code.findbugs--jsr305--com.google.code.findbugs__jsr305__1.3.9.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.commons--commons-lang3--org.apache.commons__commons-lang3__3.4.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.httpcomponents--httpcore--org.apache.httpcomponents__httpcore__4.4.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.avro--avro--org.apache.avro__avro__1.7.5.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.ant--ant-launcher--org.apache.ant__ant-launcher__1.9.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-jdbc--org.apache.hive__hive-jdbc__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.mortbay.jetty--jetty-util--org.mortbay.jetty__jetty-util__6.1.26.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.google.guava--guava--com.google.guava__guava__11.0.2.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.objenesis--objenesis--org.objenesis__objenesis__1.2.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-shims--org.apache.hive__hive-shims__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.jolbox--bonecp--com.jolbox__bonecp__0.8.0.RELEASE.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-httpclient--commons-httpclient--commons-httpclient__commons-httpclient__3.0.1.jar:file:/databricks/databricks-hive/manifest.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.jackson--jackson-core-asl--org.codehaus.jackson__jackson-core-asl__1.9.13.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-ant--org.apache.hive__hive-ant__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-cli--org.apache.hive__hive-cli__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--javax.transaction--jta--javax.transaction__jta__1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--jline--jline--jline__jline__0.9.94.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.thoughtworks.paranamer--paranamer--com.thoughtworks.paranamer__paranamer__2.8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.20S--org.apache.hive.shims__hive-shims-0.20S__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-common--org.apache.hive__hive-common__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--asm--asm--asm__asm__3.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-common-secure--org.apache.hive.shims__hive-shims-common-secure__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-common--org.apache.hive.shims__hive-shims-common__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-logging--commons-logging--commons-logging__commons-logging__1.1.3.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.antlr--stringtemplate--org.antlr__stringtemplate__3.2.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.ow2.asm--asm--org.ow2.asm__asm__4.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--antlr--antlr--antlr__antlr__2.7.7.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--asm--asm-commons--asm__asm-commons__3.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.zaxxer--HikariCP--com.zaxxer__HikariCP__2.5.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.eclipse.jetty.aggregate--jetty-all--org.eclipse.jetty.aggregate__jetty-all__7.6.0.v20120127.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.mortbay.jetty--jetty--org.mortbay.jetty__jetty__6.1.26.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.commons--commons-compress--org.apache.commons__commons-compress__1.9.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-metastore--org.apache.hive__hive-metastore__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.derby--derby--org.apache.derby__derby__10.10.1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.httpcomponents--httpclient--org.apache.httpcomponents__httpclient__4.4.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--stax--stax-api--stax__stax-api__1.0.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-api--org.apache.logging.log4j__log4j-api__2.19.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-api-jdo--org.datanucleus__datanucleus-api-jdo__4.2.4.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.ant--ant--org.apache.ant__ant__1.9.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-core--org.datanucleus__datanucleus-core__4.1.17.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-lang--commons-lang--commons-lang__commons-lang__2.4.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--junit--junit--junit__junit__3.8.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.geronimo.specs--geronimo-annotation_1.0_spec--org.apache.geronimo.specs__geronimo-annotation_1.0_spec__1.1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--oro--oro--oro__oro__2.0.8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.kryo--kryo--com.esotericsoftware.kryo__kryo__2.21.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-core--org.apache.logging.log4j__log4j-core__2.19.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.antlr--antlr-runtime--org.antlr__antlr-runtime__3.4.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.antlr--ST4--org.antlr__ST4__4.0.4.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--javax.activation--activation--javax.activation__activation__1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.thrift--libthrift--org.apache.thrift__libthrift__0.9.2.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-codec--commons-codec--commons-codec__commons-codec__1.8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.thrift--libfb303--org.apache.thrift__libfb303__0.9.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-beeline--org.apache.hive__hive-beeline__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-io--commons-io--commons-io__commons-io__2.5.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.slf4j--slf4j-api--org.slf4j__slf4j-api__2.0.6.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--javax.jdo--jdo-api--javax.jdo__jdo-api__3.0.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.20--org.apache.hive.shims__hive-shims-0.20__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-service--org.apache.hive__hive-service__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.geronimo.specs--geronimo-jaspic_1.0_spec--org.apache.geronimo.specs__geronimo-jaspic_1.0_spec__1.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--javax.transaction--transaction-api--javax.transaction__transaction-api__1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-slf4j2-impl--org.apache.logging.log4j__log4j-slf4j2-impl__2.19.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.codehaus.jackson--jackson-mapper-asl--org.codehaus.jackson__jackson-mapper-asl__1.9.13.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.mortbay.jetty--servlet-api--org.mortbay.jetty__servlet-api__2.5-20081211.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.logging.log4j--log4j-1.2-api--org.apache.logging.log4j__log4j-1.2-api__2.19.0.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--javax.jdo--org.datanucleus__javax.jdo__3.2.0-m3.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-serde--org.apache.hive__hive-serde__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.xerial.snappy--snappy-java--org.xerial.snappy__snappy-java__1.0.5.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive--hive-exec--org.apache.hive__hive-exec__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--asm--asm-tree--asm__asm-tree__3.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.velocity--velocity--org.apache.velocity__velocity__1.5.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.geronimo.specs--geronimo-jta_1.1_spec--org.apache.geronimo.specs__geronimo-jta_1.1_spec__1.1.1.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--commons-cli--commons-cli--commons-cli__commons-cli__1.2.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.apache.hive.shims--hive-shims-0.23--org.apache.hive.shims__hive-shims-0.23__0.13.1-databricks-8.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--org.datanucleus--datanucleus-rdbms--org.datanucleus__datanucleus-rdbms__4.1.19.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--javax.servlet--servlet-api--javax.servlet__servlet-api__2.5.jar:file:/databricks/databricks-hive/----ws_3_4--maven-trees--hive-metastore-databricks-log4j2--com.esotericsoftware.minlog--minlog--com.esotericsoftware.minlog__minlog__1.2.jar:file:/databricks/databricks-hive/bonecp-configs.jar"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "PoolingHiveClient: Hive metastore connection pool implementation is HikariCP"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "LocalHiveClientsPool: Create Hive Metastore client pool of size 1"
    },
    {
      "timestamp": "24/07/02 20:01:44",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:01:45",
      "level": "INFO",
      "message": "HiveClientImpl: Warehouse location for Hive client (version 0.13.1) is dbfs:/user/hive/warehouse"
    },
    {
      "timestamp": "24/07/02 20:01:45",
      "level": "INFO",
      "message": "HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore"
    },
    {
      "timestamp": "24/07/02 20:01:45",
      "level": "INFO",
      "message": "ObjectStore: ObjectStore, initialize called"
    },
    {
      "timestamp": "24/07/02 20:01:46",
      "level": "INFO",
      "message": "Persistence: Property datanucleus.fixedDatastore unknown - will be ignored"
    },
    {
      "timestamp": "24/07/02 20:01:46",
      "level": "INFO",
      "message": "Persistence: Property datanucleus.connectionPool.idleTimeout unknown - will be ignored"
    },
    {
      "timestamp": "24/07/02 20:01:46",
      "level": "INFO",
      "message": "Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored"
    },
    {
      "timestamp": "24/07/02 20:01:46",
      "level": "INFO",
      "message": "Persistence: Property datanucleus.cache.level2 unknown - will be ignored"
    },
    {
      "timestamp": "24/07/02 20:01:46",
      "level": "INFO",
      "message": "HikariDataSource: HikariPool-1 - Started."
    },
    {
      "timestamp": "24/07/02 20:01:46",
      "level": "INFO",
      "message": "HikariDataSource: HikariPool-2 - Started."
    },
    {
      "timestamp": "24/07/02 20:01:47",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:01:47",
      "level": "INFO",
      "message": "ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\""
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "ObjectStore: Initialized ObjectStore"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "HiveMetaStore: Added admin role in metastore"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "HiveMetaStore: Added public role in metastore"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "HiveMetaStore: No user is added in admin role, since config is empty"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "HiveMetaStore: 0: get_database: default"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "audit: ugi=root\tip=unknown-ip-addr\tcmd=get_database: default\t"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "WARN",
      "message": "SparkContext: The JAR file:/local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar at (spark://10.89.232.166:40595/jars/spark_transformations_0_1_jar_with_dependencies.jar,Some(/local_disk0/tmp/addedFile7620a05bd1fa4166a70386b5ab9a7e8311782591247361736346/spark_transformations_0_1_jar_with_dependencies.jar)) has been added already. Overwriting of added jar is not supported in the current version."
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Removed query with execution ID:0. Current active queries:0"
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "CurrentQueryContext: Thread Thread[WRAPPER-ReplId-7ff5f-999f3-d1c14-8,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED)."
    },
    {
      "timestamp": "24/07/02 20:01:49",
      "level": "INFO",
      "message": "ScalaDriverWrapper: setupRepl:ReplId-7ff5f-999f3-d1c14-8: finished to load"
    },
    {
      "timestamp": "24/07/02 20:01:50",
      "level": "INFO",
      "message": "QueryProfileListener: Query profile sent to logger, seq number: 0, app id: app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:01:50",
      "level": "INFO",
      "message": "ProgressReporter$: Reporting progress for running commands: 9220550251726881096_6695321926550427869_job-208451582985809-run-710061071050445-action-6316190857954834"
    },
    {
      "timestamp": "24/07/02 20:01:50",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 "
    },
    {
      "timestamp": "24/07/02 20:01:50",
      "level": "INFO",
      "message": "AsyncEventQueue: Process of event SparkListenerQueryProfileParamsReady(executionId=0, ...) by listener QueryProfileListener took 1.365253875s."
    },
    {
      "timestamp": "24/07/02 20:01:52",
      "level": "INFO",
      "message": "VirtualenvCloneHelper: Creating notebook-scoped virtualenv for 993d13e2-1b3b-41ae-a281-51ae04d51037 with python user root"
    },
    {
      "timestamp": "24/07/02 20:01:52",
      "level": "INFO",
      "message": "Utils: resolved command to be run: List(virtualenv, /local_disk0/.ephemeral_nfs/envs/pythonEnv-993d13e2-1b3b-41ae-a281-51ae04d51037, -p, /local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, --no-download, --no-setuptools, --no-wheel)"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "PythonEnvCloneHelper$: Created python virtualenv: /local_disk0/.ephemeral_nfs/envs/pythonEnv-993d13e2-1b3b-41ae-a281-51ae04d51037"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/cluster_libraries/python/bin/python, -c, import sys; dirs=[p for p in sys.path if 'package' in p]; print(' '.join(dirs)))"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "Utils: resolved command to be run: List(/local_disk0/.ephemeral_nfs/envs/pythonEnv-993d13e2-1b3b-41ae-a281-51ae04d51037/bin/python, -c, from sysconfig import get_path; print(get_path('purelib')))"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 "
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "PythonEnvCloneHelper$: Created sites.pth at /local_disk0/.ephemeral_nfs/envs/pythonEnv-993d13e2-1b3b-41ae-a281-51ae04d51037/lib/python3.10/site-packages/sites.pth"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "NotebookScopedPythonEnvManager: Time spent to start virtualenv /local_disk0/.ephemeral_nfs/envs/pythonEnv-993d13e2-1b3b-41ae-a281-51ae04d51037 is 964(ms)"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "NotebookScopedPythonEnvManager: Registered /local_disk0/.ephemeral_nfs/envs/pythonEnv-993d13e2-1b3b-41ae-a281-51ae04d51037/lib/python3.10/site-packages with the WatchService sun.nio.fs.LinuxWatchService$LinuxWatchKey@1c4986ef"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "ProgressReporter$: Added result fetcher for 9220550251726881096_6695321926550427869_job-208451582985809-run-710061071050445-action-6316190857954834"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "INFO",
      "message": "SignalUtils: Registering signal handler for INT"
    },
    {
      "timestamp": "24/07/02 20:01:53",
      "level": "WARN",
      "message": "DriverILoop: Could not determine classpath from classloader of type class jdk.internal.loader.ClassLoaders$AppClassLoader"
    },
    {
      "timestamp": "24/07/02 20:01:54",
      "level": "WARN",
      "message": "DynamicSparkConfContextImpl: Ignored update because id 1719950291714 < 1719950291714; source: ConfigFile"
    },
    {
      "timestamp": "24/07/02 20:01:54",
      "level": "INFO",
      "message": "DriverCorral: Received SAFEr configs with version 1719950291714"
    },
    {
      "timestamp": "24/07/02 20:01:54",
      "level": "INFO",
      "message": "ProgressReporter$: Reporting partial results for running commands: 9220550251726881096_6695321926550427869_job-208451582985809-run-710061071050445-action-6316190857954834"
    },
    {
      "timestamp": "24/07/02 20:01:55",
      "level": "INFO",
      "message": "DriverILoop: Set class prefix to: $linebae8fc51625e48ec97f4d0ab14599eaf"
    },
    {
      "timestamp": "24/07/02 20:01:55",
      "level": "INFO",
      "message": "DriverILoop: set ContextClassLoader"
    },
    {
      "timestamp": "24/07/02 20:01:55",
      "level": "INFO",
      "message": "DriverILoop: initialized intp"
    },
    {
      "timestamp": "24/07/02 20:02:06",
      "level": "INFO",
      "message": "ExecutionPlan: \n (2) | (3) |"
    },
    {
      "timestamp": "24/07/02 20:02:06",
      "level": "INFO",
      "message": "TransformationJob$: Current layer: 0"
    },
    {
      "timestamp": "24/07/02 20:02:06",
      "level": "INFO",
      "message": "TransformationJob$: Processing stage: 2"
    },
    {
      "timestamp": "24/07/02 20:02:06",
      "level": "INFO",
      "message": "TransformationJob$: Stage input: List()"
    },
    {
      "timestamp": "24/07/02 20:02:06",
      "level": "INFO",
      "message": "ObjectStorageReadStage: Executing stage \"2\" (operation: READ)"
    },
    {
      "timestamp": "24/07/02 20:02:06",
      "level": "RESULT",
      "message": "ObjectStorageReadStage: Reading from cos, stage 2"
    },
    {
      "timestamp": "24/07/02 20:02:07",
      "level": "INFO",
      "message": "AWSCredentialProviderList:V3: Using credentials for bucket vf-demo-data-lake from shaded.databricks.org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    },
    {
      "timestamp": "24/07/02 20:02:07",
      "level": "ERROR",
      "message": "DatabricksS3LoggingUtils$:V3: S3 request failed with com.amazonaws.services.s3.model.AmazonS3Exception: Moved Permanently; request: HEAD http://s3.amazonaws.com vf-demo-data-lake/ {} Hadoop 3.3.4, aws-sdk-java/1.12.390 Linux/5.15.0-1063-aws OpenJDK_64-Bit_Server_VM/11.0.21+9-LTS java/11.0.21 scala/2.12.15 kotlin/1.6.0 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.HeadBucketRequest; Request ID: V5ZNR56RZMXD6HXZ, Extended Request ID: KTZgxEqG6QjLNuGRc0yK2AKsfs5X/I/GFBFyREVFHFXTkV+uv5gJTyvKmpBdlGUxPGZwGVVuTZ0=, Cloud Provider: AWS, Instance ID: i-0efa059c748dd4582 (Service: Amazon S3; Status Code: 301; Error Code: 301 Moved Permanently; Request ID: V5ZNR56RZMXD6HXZ; S3 Extended Request ID: KTZgxEqG6QjLNuGRc0yK2AKsfs5X/I/GFBFyREVFHFXTkV+uv5gJTyvKmpBdlGUxPGZwGVVuTZ0=; Proxy: null), S3 Extended Request ID: KTZgxEqG6QjLNuGRc0yK2AKsfs5X/I/GFBFyREVFHFXTkV+uv5gJTyvKmpBdlGUxPGZwGVVuTZ0=; Request ID: null, Extended Request ID: null, Cloud Provider: AWS, Instance ID: i-0efa059c748dd4582\ncom.amazonaws.services.s3.model.AmazonS3Exception: Moved Permanently; request: HEAD http://s3.amazonaws.com vf-demo-data-lake/ {} Hadoop 3.3.4, aws-sdk-java/1.12.390 Linux/5.15.0-1063-aws OpenJDK_64-Bit_Server_VM/11.0.21+9-LTS java/11.0.21 scala/2.12.15 kotlin/1.6.0 vendor/Azul_Systems,_Inc. cfg/retry-mode/legacy com.amazonaws.services.s3.model.HeadBucketRequest; Request ID: V5ZNR56RZMXD6HXZ, Extended Request ID: KTZgxEqG6QjLNuGRc0yK2AKsfs5X/I/GFBFyREVFHFXTkV+uv5gJTyvKmpBdlGUxPGZwGVVuTZ0=, Cloud Provider: AWS, Instance ID: i-0efa059c748dd4582 (Service: Amazon S3; Status Code: 301; Error Code: 301 Moved Permanently; Request ID: V5ZNR56RZMXD6HXZ; S3 Extended Request ID: KTZgxEqG6QjLNuGRc0yK2AKsfs5X/I/GFBFyREVFHFXTkV+uv5gJTyvKmpBdlGUxPGZwGVVuTZ0=; Proxy: null), S3 Extended Request ID: KTZgxEqG6QjLNuGRc0yK2AKsfs5X/I/GFBFyREVFHFXTkV+uv5gJTyvKmpBdlGUxPGZwGVVuTZ0=\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6432)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.EnforcingDatabricksS3Client.getObjectMetadata(EnforcingDatabricksS3Client.scala:222)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$6(S3AFileSystem.java:2345)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:434)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:393)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2335)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2304)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3975)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3920)\n\tat shaded.databricks.org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3799)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatusNoCache(LokiS3FS.scala:84)\n\tat com.databricks.common.filesystem.LokiS3FS.getFileStatus(LokiS3FS.scala:74)\n\tat com.databricks.common.filesystem.LokiFileSystem.getFileStatus(LokiFileSystem.scala:255)\n\tat com.databricks.sql.acl.fs.CredentialScopeFileSystem.getFileStatus(CredentialScopeFileSystem.scala:290)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1760)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.$anonfun$findDeltaTableRoot$1(DeltaTable.scala:245)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:245)\n\tat com.databricks.sql.transaction.tahoe.DeltaTableUtils$.findDeltaTableRoot(DeltaTable.scala:236)\n\tat org.apache.spark.sql.DataFrameReader.liftedTree1$1(DataFrameReader.scala:260)\n\tat org.apache.spark.sql.DataFrameReader.deltaTable$lzycompute$1(DataFrameReader.scala:259)\n\tat org.apache.spark.sql.DataFrameReader.deltaTable$1(DataFrameReader.scala:259)\n\tat org.apache.spark.sql.DataFrameReader.preprocessDeltaLoading(DataFrameReader.scala:289)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:332)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:241)\n\tat by.iba.vf.spark.transformation.stage.read.ObjectStorageReadStage.read(ObjectStorageReadStage.scala:48)\n\tat by.iba.vf.spark.transformation.stage.read.ReadStage.process(ReadStage.scala:37)\n\tat by.iba.vf.spark.transformation.stage.Stage.execute(Stage.scala:36)\n\tat by.iba.vf.spark.transformation.stage.Stage.execute$(Stage.scala:34)\n\tat by.iba.vf.spark.transformation.stage.read.ReadStage.execute(ReadStage.scala:29)\n\tat by.iba.vf.spark.transformation.TransformationJob$.$anonfun$run$2(TransformationJob.scala:38)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n\tat scala.collection.immutable.Set$Set1.foreach(Set.scala:141)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n\tat by.iba.vf.spark.transformation.TransformationJob$.$anonfun$run$1(TransformationJob.scala:34)\n\tat by.iba.vf.spark.transformation.TransformationJob$.$anonfun$run$1$adapted(TransformationJob.scala:32)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat by.iba.vf.spark.transformation.TransformationJob$.run(TransformationJob.scala:32)\n\tat by.iba.vf.spark.transformation.TransformationJob$.main(TransformationJob.scala:53)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:43)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$$iw$$iw$$iw$$iw.<init>(command--1:45)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$$iw$$iw$$iw.<init>(command--1:47)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$$iw$$iw.<init>(command--1:49)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$$iw.<init>(command--1:51)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read.<init>(command--1:53)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$.<init>(command--1:57)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$read$.<clinit>(command--1)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$eval$.$print$lzycompute(<notebook>:7)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$eval$.$print(<notebook>:6)\n\tat $linebae8fc51625e48ec97f4d0ab14599eaf25.$eval.$print(<notebook>)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:223)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:236)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1413)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1366)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:236)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$34(DriverLocal.scala:1015)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$23(DriverLocal.scala:1006)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:72)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:72)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:961)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:859)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:851)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:891)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:667)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:685)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:470)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:455)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:662)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:580)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:891)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:702)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:803)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:577)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:426)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:196)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:424)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:418)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:577)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:487)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:296)\n\tat java.base/java.lang.Thread.run(Thread.java:829)"
    },
    {
      "timestamp": "24/07/02 20:02:10",
      "level": "INFO",
      "message": "InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 1; threshold: 32"
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "InMemoryFileIndex: Start listing leaf files and directories. Size of Paths: 0; threshold: 32"
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "InMemoryFileIndex: It took 578 ms to list leaf files for 1 paths."
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.019446 ms."
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.061329 ms."
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.101931 ms."
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Added query with execution ID:1. Current active queries:1"
    },
    {
      "timestamp": "24/07/02 20:02:11",
      "level": "INFO",
      "message": "AdaptiveParallelism: Updating parallelism using instant cluster load. Old parallelism: 2, Total cores: 2, Current load: 1, Current Avg load: 0, New parallelism: 2"
    },
    {
      "timestamp": "24/07/02 20:02:12",
      "level": "INFO",
      "message": "FileSourceStrategy: Pushed Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:12",
      "level": "INFO",
      "message": "FileSourceStrategy: Post-Scan Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_0 stored as values in memory (estimated size 301.2 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_1 stored as values in memory (estimated size 112.2 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 18.3 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.89.232.166:45711 (size: 18.3 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "SparkContext: Created broadcast 1 from writeExternal at ObjectOutputStream.java:1450"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.89.232.166:45711 (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "SparkContext: Created broadcast 0 from execute at photonPhysicalOperators.scala:442"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "FileSourceScanExec: Planning scan with bin packing, max split size: 6291694 bytes, max partition size: 6291694, open cost is considered as scanning 4194304 bytes."
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "SparkContext: Starting job: show at ReadStage.scala:41"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "DAGScheduler: Got job 0 (show at ReadStage.scala:41) with 1 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 0 (show at ReadStage.scala:41)"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at show at ReadStage.scala:41), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:13",
      "level": "INFO",
      "message": "DAGScheduler: submitMissingTasks(ResultStage 0): 1 / 2 partitions missing, starting partial re-computation"
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at ReadStage.scala:41) (first 15 tasks are for partitions Vector(0))"
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 0.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "WARN",
      "message": "FairSchedulableBuilder: A job was submitted with scheduler pool 9220550251726881096, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 9220550251726881096. Created 9220550251726881096 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)"
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:14",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 0.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:17",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:20",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:23",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:26",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:29",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:32",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:35",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:35",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.89.238.116:32951 (size: 18.3 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:38",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:41",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:44",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:46",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.89.238.116:32951 (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 33234 ms on 10.89.238.116 (executor 0) (1/1)"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 0 (show at ReadStage.scala:41) finished in 33.572 s"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Job 0 finished: show at ReadStage.scala:41, took 33.854425 s"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "SparkContext: Starting job: show at ReadStage.scala:41"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Got job 1 (show at ReadStage.scala:41) with 1 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 1 (show at ReadStage.scala:41)"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at show at ReadStage.scala:41), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: submitMissingTasks(ResultStage 1): 1 / 2 partitions missing, starting partial re-computation"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at show at ReadStage.scala:41) (first 15 tasks are for partitions Vector(1))"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 1.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:47",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (10.89.238.116, executor 0, partition 1, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 272 ms on 10.89.238.116 (executor 0) (1/1)"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 1 (show at ReadStage.scala:41) finished in 0.315 s"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "DAGScheduler: Job 1 finished: show at ReadStage.scala:41, took 0.349298 s"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.062696 ms."
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Removed query with execution ID:1. Current active queries:0"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "RESULT",
      "message": "ObjectStorageReadStage: Read data sample(top 10 rows):\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                                                                                                                                                                           |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|My flight from Sofia to Stockholm was delayed for 8 hours due to the snowfall in Helsinki!! And I had to wait in that airport of Krakow! Support specialist John Doe didn't suggest me alternative flights! I am very disappointed with the company and the quality of support!!|\n|It was wounderful fliw from London to Paris. What I liked most are flight attendands, they were very polite.                                                                                                                                                                    |\n|I will never flight with this airlines! My buggage was delayed! I didn't come to Berlin on time!                                                                                                                                                                                |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.114924 ms."
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Added query with execution ID:2. Current active queries:1"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "AdaptiveParallelism: Updating parallelism using instant cluster load. Old parallelism: 2, Total cores: 2, Current load: 1, Current Avg load: 1, New parallelism: 2"
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "FileSourceStrategy: Pushed Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "FileSourceStrategy: Post-Scan Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:48",
      "level": "INFO",
      "message": "QueryProfileListener: Query profile sent to logger, seq number: 1, app id: app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_2 stored as values in memory (estimated size 301.2 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.89.232.166:45711 (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "SparkContext: Created broadcast 2 from execute at photonPhysicalOperators.scala:442"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "FileSourceScanExec: Planning scan with bin packing, max split size: 6291694 bytes, max partition size: 6291694, open cost is considered as scanning 4194304 bytes."
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Registering RDD 10 (mapPartitionsInternal at PhotonExec.scala:483) as input to shuffle 0"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Got map stage job 2 (submitMapStage at PhotonExec.scala:495) with 2 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ShuffleMapStage 2 (mapPartitionsInternal at PhotonExec.scala:483)"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at mapPartitionsInternal at PhotonExec.scala:483), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at mapPartitionsInternal at PhotonExec.scala:483) (first 15 tasks are for partitions Vector(0, 1))"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 2.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (10.89.238.116, executor 0, partition 1, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.89.238.116:32951 (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:49",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 500 ms on 10.89.238.116 (executor 0) (1/2)"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 637 ms on 10.89.238.116 (executor 0) (2/2)"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: ShuffleMapStage 2 (mapPartitionsInternal at PhotonExec.scala:483) finished in 0.693 s"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: looking for newly runnable stages"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: running: Set()"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: waiting: Set()"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: failed: Set()"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "CodeGenerator: Code generated in 295.504954 ms"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "SparkContext: Starting job: $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: Got job 3 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) with 1 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 4 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63)"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[14] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[14] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) (first 15 tasks are for partitions Vector(0))"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 4.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_4.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:50",
      "level": "INFO",
      "message": "MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.89.238.116:39804"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 319 ms on 10.89.238.116 (executor 0) (1/1)"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 4 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) finished in 0.351 s"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "DAGScheduler: Job 3 finished: $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63, took 0.381052 s"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Removed query with execution ID:2. Current active queries:0"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "RESULT",
      "message": "ObjectStorageReadStage: Total number of rows read: 3"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TransformationJob$: Resulting DataFrame schema:\nroot\n |-- value: string (nullable = true)\n"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TransformationJob$: Current layer: 1"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TransformationJob$: Processing stage: 3"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "TransformationJob$: Stage input: List(2)"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "ObjectStorageWriteStage: Executing stage \"3\" (operation: WRITE)"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "RESULT",
      "message": "ObjectStorageWriteStage: Writing to cos, stage 3"
    },
    {
      "timestamp": "24/07/02 20:02:51",
      "level": "INFO",
      "message": "QueryProfileListener: Query profile sent to logger, seq number: 2, app id: app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:02:52",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 1.784387 ms."
    },
    {
      "timestamp": "24/07/02 20:02:52",
      "level": "INFO",
      "message": "CurrentQueryContext: Thread Thread[WRAPPER-ReplId-7ff5f-999f3-d1c14-8,5,main]: Setting current query category as an executable command (Command is a class org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand)."
    },
    {
      "timestamp": "24/07/02 20:02:52",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Added query with execution ID:3. Current active queries:1"
    },
    {
      "timestamp": "24/07/02 20:02:52",
      "level": "INFO",
      "message": "AdaptiveParallelism: Updating parallelism using instant cluster load. Old parallelism: 2, Total cores: 2, Current load: 1, Current Avg load: 1, New parallelism: 2"
    },
    {
      "timestamp": "24/07/02 20:02:52",
      "level": "INFO",
      "message": "FileSourceStrategy: Pushed Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:52",
      "level": "INFO",
      "message": "FileSourceStrategy: Post-Scan Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "BlockManagerInfo: Removed broadcast_2_piece0 on 10.89.238.116:32951 in memory (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "BlockManagerInfo: Removed broadcast_0_piece0 on 10.89.238.116:32951 in memory (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "BlockManagerInfo: Removed broadcast_0_piece0 on 10.89.232.166:45711 in memory (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "BlockManagerInfo: Removed broadcast_2_piece0 on 10.89.232.166:45711 in memory (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "CodeGenerator: Code generated in 64.029741 ms"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_3 stored as values in memory (estimated size 301.2 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.89.232.166:45711 (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "SparkContext: Created broadcast 3 from execute at photonPhysicalOperators.scala:442"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "FileSourceScanExec: Planning scan with bin packing, max split size: 6291694 bytes, max partition size: 6291694, open cost is considered as scanning 4194304 bytes."
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "SparkContext: Starting job: save at ObjectStorageWriteStage.scala:44"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "DAGScheduler: Got job 4 (save at ObjectStorageWriteStage.scala:44) with 2 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 5 (save at ObjectStorageWriteStage.scala:44)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at save at ObjectStorageWriteStage.scala:44), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 2 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at save at ObjectStorageWriteStage.scala:44) (first 15 tasks are for partitions Vector(0, 1))"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 5.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "WARN",
      "message": "FairSchedulableBuilder: A job was submitted with scheduler pool 9220550251726881096, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 9220550251726881096. Created 9220550251726881096 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_5.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 1.0 in stage 5.0 (TID 6) (10.89.238.116, executor 0, partition 1, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:53",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.89.238.116:32951 (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:54",
      "level": "WARN",
      "message": "DynamicSparkConfContextImpl: Ignored update because id 1719950291714 < 1719950291714; source: ConfigFile"
    },
    {
      "timestamp": "24/07/02 20:02:54",
      "level": "INFO",
      "message": "DriverCorral: Received SAFEr configs with version 1719950291714"
    },
    {
      "timestamp": "24/07/02 20:02:54",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 1.0 in stage 5.0 (TID 6) in 1570 ms on 10.89.238.116 (executor 0) (1/2)"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1625 ms on 10.89.238.116 (executor 0) (2/2)"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 5 (save at ObjectStorageWriteStage.scala:44) finished in 1.721 s"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "DAGScheduler: Job 4 finished: save at ObjectStorageWriteStage.scala:44, took 1.743875 s"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "FileFormatWriter: Start to commit write Job ea8c204e-5844-476d-bdb5-2fa8f8478060."
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "DirectoryAtomicCommitProtocol: Committing job 5c08b8a5-2410-4e7b-abdc-f9725f981416"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3AFileSystem:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[s3a://vf-demo-data-lake/text_copy/_committed_868908028251208642] Creating output stream; permission: { masked: rw-r--r--, unmasked: rw-rw-rw- }, overwrite: false, bufferSize: 65536"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3AFileSystem:V3: spark.databricks.io.parquet.verifyChecksumOnWrite.enabled is disabled"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3ABlockOutputStream:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[text_copy/_committed_868908028251208642] Closing stream; size: 265"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3ABlockOutputStream:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[text_copy/_committed_868908028251208642] Upload complete; size: 265"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3AFileSystem:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[s3a://vf-demo-data-lake/text_copy/_SUCCESS] Creating output stream; permission: { masked: rw-r--r--, unmasked: rw-rw-rw- }, overwrite: true, bufferSize: 65536"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3AFileSystem:V3: spark.databricks.io.parquet.verifyChecksumOnWrite.enabled is disabled"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3ABlockOutputStream:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[text_copy/_SUCCESS] Closing stream; size: 0"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3ABlockOutputStream:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[text_copy/_SUCCESS] Upload complete; size: 0"
    },
    {
      "timestamp": "24/07/02 20:02:55",
      "level": "INFO",
      "message": "S3AFileSystem:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[s3a://vf-demo-data-lake/text_copy/_SUCCESS] Creating output stream; permission: { masked: rw-r--r--, unmasked: rw-rw-rw- }, overwrite: true, bufferSize: 65536"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "S3AFileSystem:V3: spark.databricks.io.parquet.verifyChecksumOnWrite.enabled is disabled"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "S3ABlockOutputStream:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[text_copy/_SUCCESS] Closing stream; size: 0"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "S3ABlockOutputStream:V3: FS_OP_CREATE BUCKET[vf-demo-data-lake] FILE[text_copy/_SUCCESS] Upload complete; size: 0"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DirectoryAtomicCommitProtocol: Job commit completed for 5c08b8a5-2410-4e7b-abdc-f9725f981416"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DirectoryAtomicCommitProtocol: Auto-vacuuming directories updated by 5c08b8a5-2410-4e7b-abdc-f9725f981416"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DirectoryAtomicCommitProtocol: Started VACUUM on Set(s3a://vf-demo-data-lake/text_copy) with data horizon 1719777776229 (now - 48.0 hours), metadata horizon 1719948776229 (now - 0.5 hours)"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 1, Old Ema: 1.0, New Ema: 1.0 "
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "FileFormatWriter: Write Job ea8c204e-5844-476d-bdb5-2fa8f8478060 committed. Elapsed time: 1758 ms."
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "FileFormatWriter: Finished processing stats for write job ea8c204e-5844-476d-bdb5-2fa8f8478060."
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Removed query with execution ID:3. Current active queries:0"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "CurrentQueryContext: Thread Thread[WRAPPER-ReplId-7ff5f-999f3-d1c14-8,5,main]: current category Some(EXECUTABLE_COMMAND), restoring to previous category Some(UNDETERMINED)."
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "QueryProfileListener: Query profile sent to logger, seq number: 3, app id: app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.0625 ms."
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.093745 ms."
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Added query with execution ID:4. Current active queries:1"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "AdaptiveParallelism: Updating parallelism using instant cluster load. Old parallelism: 2, Total cores: 2, Current load: 1, Current Avg load: 1, New parallelism: 2"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "FileSourceStrategy: Pushed Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "FileSourceStrategy: Post-Scan Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_4 stored as values in memory (estimated size 301.2 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.89.232.166:45711 (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "SparkContext: Created broadcast 4 from execute at photonPhysicalOperators.scala:442"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "FileSourceScanExec: Planning scan with bin packing, max split size: 6291694 bytes, max partition size: 6291694, open cost is considered as scanning 4194304 bytes."
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "SparkContext: Starting job: show at WriteStage.scala:49"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DAGScheduler: Got job 5 (show at WriteStage.scala:49) with 1 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 6 (show at WriteStage.scala:49)"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[25] at show at WriteStage.scala:49), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:56",
      "level": "INFO",
      "message": "DAGScheduler: submitMissingTasks(ResultStage 6): 1 / 2 partitions missing, starting partial re-computation"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[25] at show at WriteStage.scala:49) (first 15 tasks are for partitions Vector(0))"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 6.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "WARN",
      "message": "FairSchedulableBuilder: A job was submitted with scheduler pool 9220550251726881096, which has not been configured. This can happen when the file that pools are read from isn't set, or when that file doesn't contain 9220550251726881096. Created 9220550251726881096 with default configuration (schedulingMode: FIFO, minShare: 0, weight: 1)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_6.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 6.0 (TID 7) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.89.238.116:32951 (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 6.0 (TID 7) in 302 ms on 10.89.238.116 (executor 0) (1/1)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 6 (show at WriteStage.scala:49) finished in 0.327 s"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Job 5 finished: show at WriteStage.scala:49, took 0.348642 s"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "SparkContext: Starting job: show at WriteStage.scala:49"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Got job 6 (show at WriteStage.scala:49) with 1 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 7 (show at WriteStage.scala:49)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[25] at show at WriteStage.scala:49), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: submitMissingTasks(ResultStage 7): 1 / 2 partitions missing, starting partial re-computation"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[25] at show at WriteStage.scala:49) (first 15 tasks are for partitions Vector(1))"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 7.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_7.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 7.0 (TID 8) (10.89.238.116, executor 0, partition 1, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 7.0 (TID 8) in 193 ms on 10.89.238.116 (executor 0) (1/1)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 7 (show at WriteStage.scala:49) finished in 0.221 s"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Job 6 finished: show at WriteStage.scala:49, took 0.237610 s"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.058851 ms."
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Removed query with execution ID:4. Current active queries:0"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "RESULT",
      "message": "ObjectStorageWriteStage: Written data sample(top 10 rows):\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|value                                                                                                                                                                                                                                                                           |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|My flight from Sofia to Stockholm was delayed for 8 hours due to the snowfall in Helsinki!! And I had to wait in that airport of Krakow! Support specialist John Doe didn't suggest me alternative flights! I am very disappointed with the company and the quality of support!!|\n|It was wounderful fliw from London to Paris. What I liked most are flight attendands, they were very polite.                                                                                                                                                                    |\n|I will never flight with this airlines! My buggage was delayed! I didn't come to Berlin on time!                                                                                                                                                                                |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "QueryProfileListener: Query profile sent to logger, seq number: 4, app id: app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "HiveUnityCatalogCheckRule: Collecting SQL actions took 0.056467 ms."
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Added query with execution ID:5. Current active queries:1"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "AdaptiveParallelism: Updating parallelism using instant cluster load. Old parallelism: 2, Total cores: 2, Current load: 1, Current Avg load: 1, New parallelism: 2"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "FileSourceStrategy: Pushed Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "FileSourceStrategy: Post-Scan Filters: "
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_5 stored as values in memory (estimated size 301.2 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.0 KiB, free 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.89.232.166:45711 (size: 15.0 KiB, free: 10.9 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "SparkContext: Created broadcast 5 from execute at photonPhysicalOperators.scala:442"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "FileSourceScanExec: Planning scan with bin packing, max split size: 6291694 bytes, max partition size: 6291694, open cost is considered as scanning 4194304 bytes."
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Registering RDD 31 (mapPartitionsInternal at PhotonExec.scala:483) as input to shuffle 1"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Got map stage job 7 (submitMapStage at PhotonExec.scala:495) with 2 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ShuffleMapStage 8 (mapPartitionsInternal at PhotonExec.scala:483)"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List()"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[31] at mapPartitionsInternal at PhotonExec.scala:483), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[31] at mapPartitionsInternal at PhotonExec.scala:483) (first 15 tasks are for partitions Vector(0, 1))"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 8.0 with 2 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 8.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_8.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 8.0 (TID 9) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 1.0 in stage 8.0 (TID 10) (10.89.238.116, executor 0, partition 1, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:57",
      "level": "INFO",
      "message": "BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.89.238.116:32951 (size: 15.0 KiB, free: 7.6 GiB)"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 1.0 in stage 8.0 (TID 10) in 191 ms on 10.89.238.116 (executor 0) (1/2)"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 8.0 (TID 9) in 279 ms on 10.89.238.116 (executor 0) (2/2)"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: ShuffleMapStage 8 (mapPartitionsInternal at PhotonExec.scala:483) finished in 0.305 s"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: looking for newly runnable stages"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: running: Set()"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: waiting: Set()"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: failed: Set()"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "CodeGenerator: Code generated in 30.053366 ms"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "SparkContext: Starting job: $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Got job 8 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) with 1 output partitions"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Final stage: ResultStage 10 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63)"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Missing parents: List()"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63), which has no missing parents"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[35] at $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) (first 15 tasks are for partitions Vector(0))"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSetManager: TaskSet 10.0 using PreferredLocationsV1"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "FairSchedulableBuilder: Added task set TaskSet_10.0 tasks to pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSetManager: Starting task 0.0 in stage 10.0 (TID 11) (10.89.238.116, executor 0, partition 0, PROCESS_LOCAL, "
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.89.238.116:39804"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSetManager: Finished task 0.0 in stage 10.0 (TID 11) in 44 ms on 10.89.238.116 (executor 0) (1/1)"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 9220550251726881096"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: ResultStage 10 ($anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63) finished in 0.070 s"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "DAGScheduler: Job 8 finished: $anonfun$withThreadLocalCaptured$5 at LexicalThreadLocal.scala:63, took 0.080463 s"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "ClusterLoadMonitor: Removed query with execution ID:5. Current active queries:0"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "RESULT",
      "message": "ObjectStorageWriteStage: Total number of rows written: 3"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "ProgressReporter$: Removed result fetcher for 9220550251726881096_6695321926550427869_job-208451582985809-run-710061071050445-action-6316190857954834"
    },
    {
      "timestamp": "24/07/02 20:02:58",
      "level": "INFO",
      "message": "QueryProfileListener: Query profile sent to logger, seq number: 5, app id: app-20240702200036-0000"
    },
    {
      "timestamp": "24/07/02 20:02:59",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 1.0, New Ema: 0.85 "
    },
    {
      "timestamp": "24/07/02 20:03:02",
      "level": "INFO",
      "message": "DriverCorral$: Cleaning the wrapper ReplId-7ff5f-999f3-d1c14-8 (currently in status Idle(ReplId-7ff5f-999f3-d1c14-8))"
    },
    {
      "timestamp": "24/07/02 20:03:02",
      "level": "INFO",
      "message": "DriverCorral$: sending shutdown signal for REPL ReplId-7ff5f-999f3-d1c14-8"
    },
    {
      "timestamp": "24/07/02 20:03:02",
      "level": "INFO",
      "message": "DriverCorral$: sending the interrupt signal for REPL ReplId-7ff5f-999f3-d1c14-8"
    },
    {
      "timestamp": "24/07/02 20:03:02",
      "level": "INFO",
      "message": "DriverCorral$: waiting for localThread to stop for REPL ReplId-7ff5f-999f3-d1c14-8"
    },
    {
      "timestamp": "24/07/02 20:03:02",
      "level": "INFO",
      "message": "DriverCorral$: ReplId-7ff5f-999f3-d1c14-8 successfully discarded"
    },
    {
      "timestamp": "24/07/02 20:03:02",
      "level": "INFO",
      "message": "ClusterLoadAvgHelper: Current cluster load: 0, Old Ema: 0.85, New Ema: 0.0 "
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "StandaloneAppClient$ClientEndpoint: Executor updated: app-20240702200036-0000/0 is now LOST (worker lost: 10.89.238.116:33589 got disassociated)"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "StandaloneSchedulerBackend: Executor app-20240702200036-0000/0 removed: worker lost: 10.89.238.116:33589 got disassociated"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "StandaloneAppClient$ClientEndpoint: Master removed worker worker-20240702200004-10.89.238.116-33589: 10.89.238.116:33589 got disassociated"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "StandaloneSchedulerBackend: Worker worker-20240702200004-10.89.238.116-33589 removed: 10.89.238.116:33589 got disassociated"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "ERROR",
      "message": "TaskSchedulerImpl: Lost executor 0 on 10.89.238.116: worker lost: 10.89.238.116:33589 got disassociated"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "DAGScheduler: Executor lost: 0 (epoch 2)"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "TaskSchedulerImpl: Handle removed worker worker-20240702200004-10.89.238.116-33589: 10.89.238.116:33589 got disassociated"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "BlockManagerMasterEndpoint: Trying to remove executor 0 from BlockManagerMaster."
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "BlockManagerMasterEndpoint: Removing block manager BlockManagerId(0, 10.89.238.116, 32951, None)"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "BlockManagerMaster: Removed 0 successfully in removeExecutor"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "DAGScheduler: Shuffle files lost for host: 10.89.238.116 (epoch 2)"
    },
    {
      "timestamp": "24/07/02 20:03:03",
      "level": "INFO",
      "message": "DAGScheduler: Shuffle files lost for worker worker-20240702200004-10.89.238.116-33589 on host 10.89.238.116"
    }
  ]
}